{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download api token from: https://www.kaggle.com/settings -> and upload here:\n",
    "from google.colab import files\n",
    "\n",
    "files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir ~/.kaggle\n",
    "!cp kaggle.json ~/.kaggle/\n",
    "!chmod 600 ~/.kaggle/kaggle.json\n",
    "!kaggle datasets download -d nguyentuannguyen/optiver-trading-at-the-close\n",
    "!unzip optiver-trading-at-the-close.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = pd.read_csv('/content/optiver-trading-at-the-close/train.csv')\n",
    "train_dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5237980, 17)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = pd.read_csv('data/optiver-trading-at-the-close/train.csv')\n",
    "train_dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection and engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5237760, 17)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "stock_id                     int64\n",
       "date_id                      int64\n",
       "seconds_in_bucket            int64\n",
       "imbalance_size             float32\n",
       "imbalance_buy_sell_flag      int64\n",
       "reference_price            float32\n",
       "matched_size               float32\n",
       "far_price                  float32\n",
       "near_price                 float32\n",
       "bid_price                  float32\n",
       "bid_size                   float32\n",
       "ask_price                  float32\n",
       "ask_size                   float32\n",
       "wap                        float32\n",
       "target                     float32\n",
       "time_id                      int64\n",
       "row_id                      object\n",
       "dtype: object"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List of columns to convert to float32\n",
    "columns_to_convert = ['reference_price', 'matched_size', 'far_price', 'near_price',\n",
    "                      'bid_price', 'bid_size', 'ask_price', 'ask_size',\n",
    "                      'wap', 'target', 'imbalance_size']\n",
    "\n",
    "# Convert the columns to float32\n",
    "train_dataset[columns_to_convert] = train_dataset[columns_to_convert].astype('float32')\n",
    "train_dataset.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import QuantileTransformer\n",
    "# Applying Quantile Transformation to follow a normal distribution\n",
    "\n",
    "quantile_transformer = QuantileTransformer(output_distribution='normal', random_state=0)\n",
    "train_dataset['Quantile_imbalance_size'] = quantile_transformer.fit_transform(train_dataset['imbalance_size'].values.reshape(-1, 1)).flatten()\n",
    "train_dataset['Quantile_far_price'] = quantile_transformer.fit_transform(train_dataset['far_price'].values.reshape(-1, 1)).flatten()\n",
    "\n",
    "# bid_size\n",
    "train_dataset['Quantile_bid_size'] = quantile_transformer.fit_transform(train_dataset['bid_size'].values.reshape(-1, 1)).flatten()\n",
    "\n",
    "# ask_size\n",
    "train_dataset['Quantile_ask_size'] = quantile_transformer.fit_transform(train_dataset['ask_size'].values.reshape(-1, 1)).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original features\n",
    "features = ['imbalance_size', 'far_price', 'bid_size', 'ask_size']\n",
    "\n",
    "# drop features\n",
    "train_dataset = train_dataset.drop(features, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique</th>\n",
       "      <th>cardinality</th>\n",
       "      <th>with_null</th>\n",
       "      <th>null_pct</th>\n",
       "      <th>dtype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>stock_id</th>\n",
       "      <td>False</td>\n",
       "      <td>200</td>\n",
       "      <td>False</td>\n",
       "      <td>0.00</td>\n",
       "      <td>int64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date_id</th>\n",
       "      <td>False</td>\n",
       "      <td>481</td>\n",
       "      <td>False</td>\n",
       "      <td>0.00</td>\n",
       "      <td>int64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>seconds_in_bucket</th>\n",
       "      <td>False</td>\n",
       "      <td>55</td>\n",
       "      <td>False</td>\n",
       "      <td>0.00</td>\n",
       "      <td>int64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>imbalance_buy_sell_flag</th>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>0.00</td>\n",
       "      <td>int64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reference_price</th>\n",
       "      <td>False</td>\n",
       "      <td>28741</td>\n",
       "      <td>True</td>\n",
       "      <td>0.00</td>\n",
       "      <td>float32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>matched_size</th>\n",
       "      <td>False</td>\n",
       "      <td>2875627</td>\n",
       "      <td>True</td>\n",
       "      <td>0.00</td>\n",
       "      <td>float32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>near_price</th>\n",
       "      <td>False</td>\n",
       "      <td>84625</td>\n",
       "      <td>True</td>\n",
       "      <td>54.55</td>\n",
       "      <td>float32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bid_price</th>\n",
       "      <td>False</td>\n",
       "      <td>28313</td>\n",
       "      <td>True</td>\n",
       "      <td>0.00</td>\n",
       "      <td>float32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ask_price</th>\n",
       "      <td>False</td>\n",
       "      <td>28266</td>\n",
       "      <td>True</td>\n",
       "      <td>0.00</td>\n",
       "      <td>float32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wap</th>\n",
       "      <td>False</td>\n",
       "      <td>31506</td>\n",
       "      <td>True</td>\n",
       "      <td>0.00</td>\n",
       "      <td>float32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>target</th>\n",
       "      <td>False</td>\n",
       "      <td>15934</td>\n",
       "      <td>True</td>\n",
       "      <td>0.00</td>\n",
       "      <td>float32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time_id</th>\n",
       "      <td>False</td>\n",
       "      <td>26455</td>\n",
       "      <td>False</td>\n",
       "      <td>0.00</td>\n",
       "      <td>int64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>row_id</th>\n",
       "      <td>True</td>\n",
       "      <td>5237980</td>\n",
       "      <td>False</td>\n",
       "      <td>0.00</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Quantile_imbalance_size</th>\n",
       "      <td>False</td>\n",
       "      <td>2682391</td>\n",
       "      <td>True</td>\n",
       "      <td>0.00</td>\n",
       "      <td>float32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Quantile_far_price</th>\n",
       "      <td>False</td>\n",
       "      <td>95275</td>\n",
       "      <td>True</td>\n",
       "      <td>55.26</td>\n",
       "      <td>float32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Quantile_bid_size</th>\n",
       "      <td>False</td>\n",
       "      <td>2485546</td>\n",
       "      <td>False</td>\n",
       "      <td>0.00</td>\n",
       "      <td>float32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Quantile_ask_size</th>\n",
       "      <td>False</td>\n",
       "      <td>2512524</td>\n",
       "      <td>False</td>\n",
       "      <td>0.00</td>\n",
       "      <td>float32</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         unique  cardinality  with_null  null_pct    dtype\n",
       "stock_id                  False          200      False      0.00    int64\n",
       "date_id                   False          481      False      0.00    int64\n",
       "seconds_in_bucket         False           55      False      0.00    int64\n",
       "imbalance_buy_sell_flag   False            3      False      0.00    int64\n",
       "reference_price           False        28741       True      0.00  float32\n",
       "matched_size              False      2875627       True      0.00  float32\n",
       "near_price                False        84625       True     54.55  float32\n",
       "bid_price                 False        28313       True      0.00  float32\n",
       "ask_price                 False        28266       True      0.00  float32\n",
       "wap                       False        31506       True      0.00  float32\n",
       "target                    False        15934       True      0.00  float32\n",
       "time_id                   False        26455      False      0.00    int64\n",
       "row_id                     True      5237980      False      0.00   object\n",
       "Quantile_imbalance_size   False      2682391       True      0.00  float32\n",
       "Quantile_far_price        False        95275       True     55.26  float32\n",
       "Quantile_bid_size         False      2485546      False      0.00  float32\n",
       "Quantile_ask_size         False      2512524      False      0.00  float32"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def inspect_columns(df):\n",
    "    result = pd.DataFrame({\n",
    "        'unique': df.nunique() == len(df),\n",
    "        'cardinality': df.nunique(),\n",
    "        'with_null': df.isna().any(),\n",
    "#         'null_num': df.isnull().sum(),\n",
    "        'null_pct': round((df.isnull().sum() / len(df)) * 100, 2),\n",
    "#         '1st_row': df.iloc[0],\n",
    "#         'last_row': df.iloc[-1],\n",
    "        'dtype': df.dtypes\n",
    "    })\n",
    "    return result\n",
    "\n",
    "inspect_columns(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset['volume'] = train_dataset['Quantile_ask_size'] + train_dataset['Quantile_bid_size']\n",
    "train_dataset['volume_norm'] = (train_dataset['volume'] - train_dataset['volume'].mean()) / train_dataset['volume'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5237760, 19)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = train_dataset.copy()\n",
    "\n",
    "def drop_missing_values(df):\n",
    "    # drop the missing values of wap\n",
    "    df = df.dropna(subset=['wap'])\n",
    "    return df\n",
    "# drop the missing values of wap\n",
    "df_train = drop_missing_values(df_train)\n",
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### keep only the features selected in \"trading_at_the_close_visualisation\"\n",
    " ['stock_id' 'date_id' 'seconds_in_bucket' 'imbalance_size'\n",
    " 'reference_price' 'matched_size' 'far_price' 'near_price' 'bid_price'\n",
    " 'bid_size' 'ask_price' 'ask_size' 'wap' 'row_id' 'date_id_week'\n",
    " 'date_id_seconds' 'total_volume' 'near_far_ratio' 'near_far_imbalance'\n",
    " 'ask_mat_ratio' 'bid_mat_ratio' 'ask_ref_ratio'\n",
    " 'imbalance_continuous_ratio' 'ask_wap_diff' 'all_prices_skew'\n",
    " 'all_sizes_skew' 'all_prices_kurt' 'all_sizes_kurt'\n",
    " 'imbalance_buy_sell_flag_cumsum' 'imbalance_buy_sell_flag_0'\n",
    " 'imbalance_buy_sell_flag_1']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 2.39 s\n",
      "Wall time: 9.25 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stock_id</th>\n",
       "      <th>date_id</th>\n",
       "      <th>seconds_in_bucket</th>\n",
       "      <th>imbalance_buy_sell_flag</th>\n",
       "      <th>reference_price</th>\n",
       "      <th>matched_size</th>\n",
       "      <th>near_price</th>\n",
       "      <th>bid_price</th>\n",
       "      <th>ask_price</th>\n",
       "      <th>wap</th>\n",
       "      <th>...</th>\n",
       "      <th>ask_wap_diff</th>\n",
       "      <th>imbalance_buy_sell_flag_cumsum</th>\n",
       "      <th>all_prices_mean</th>\n",
       "      <th>all_sizes_mean</th>\n",
       "      <th>all_prices_std</th>\n",
       "      <th>all_sizes_std</th>\n",
       "      <th>all_prices_skew</th>\n",
       "      <th>all_sizes_skew</th>\n",
       "      <th>all_prices_kurt</th>\n",
       "      <th>all_sizes_kurt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5237975</th>\n",
       "      <td>195</td>\n",
       "      <td>480</td>\n",
       "      <td>540</td>\n",
       "      <td>-1</td>\n",
       "      <td>1.000317</td>\n",
       "      <td>28280362.0</td>\n",
       "      <td>0.999734</td>\n",
       "      <td>1.000317</td>\n",
       "      <td>1.000434</td>\n",
       "      <td>1.000328</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000106</td>\n",
       "      <td>-54</td>\n",
       "      <td>0.830386</td>\n",
       "      <td>7070091.00</td>\n",
       "      <td>0.416020</td>\n",
       "      <td>14140180.0</td>\n",
       "      <td>-2.449488</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.999992</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5237976</th>\n",
       "      <td>196</td>\n",
       "      <td>480</td>\n",
       "      <td>540</td>\n",
       "      <td>-1</td>\n",
       "      <td>1.000643</td>\n",
       "      <td>9187699.0</td>\n",
       "      <td>1.000386</td>\n",
       "      <td>1.000643</td>\n",
       "      <td>1.000900</td>\n",
       "      <td>1.000819</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>-55</td>\n",
       "      <td>0.847850</td>\n",
       "      <td>2296925.50</td>\n",
       "      <td>0.374351</td>\n",
       "      <td>4593849.0</td>\n",
       "      <td>-2.449489</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.999995</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5237977</th>\n",
       "      <td>197</td>\n",
       "      <td>480</td>\n",
       "      <td>540</td>\n",
       "      <td>0</td>\n",
       "      <td>0.995789</td>\n",
       "      <td>12725436.0</td>\n",
       "      <td>0.995789</td>\n",
       "      <td>0.995789</td>\n",
       "      <td>0.995883</td>\n",
       "      <td>0.995797</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000086</td>\n",
       "      <td>-54</td>\n",
       "      <td>0.705909</td>\n",
       "      <td>3181358.25</td>\n",
       "      <td>0.710108</td>\n",
       "      <td>6362718.5</td>\n",
       "      <td>-2.449490</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5237978</th>\n",
       "      <td>198</td>\n",
       "      <td>480</td>\n",
       "      <td>540</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999210</td>\n",
       "      <td>94773272.0</td>\n",
       "      <td>0.999210</td>\n",
       "      <td>0.998970</td>\n",
       "      <td>0.999210</td>\n",
       "      <td>0.999008</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000202</td>\n",
       "      <td>28</td>\n",
       "      <td>0.809469</td>\n",
       "      <td>23693318.00</td>\n",
       "      <td>0.464552</td>\n",
       "      <td>47386636.0</td>\n",
       "      <td>-2.449490</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.999999</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5237979</th>\n",
       "      <td>199</td>\n",
       "      <td>480</td>\n",
       "      <td>540</td>\n",
       "      <td>-1</td>\n",
       "      <td>1.002129</td>\n",
       "      <td>24073678.0</td>\n",
       "      <td>1.001494</td>\n",
       "      <td>1.002129</td>\n",
       "      <td>1.002447</td>\n",
       "      <td>1.002274</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000173</td>\n",
       "      <td>-35</td>\n",
       "      <td>0.882249</td>\n",
       "      <td>6018420.50</td>\n",
       "      <td>0.293562</td>\n",
       "      <td>12036838.0</td>\n",
       "      <td>-2.449483</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.999975</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 43 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         stock_id  date_id  seconds_in_bucket  imbalance_buy_sell_flag  \\\n",
       "5237975       195      480                540                       -1   \n",
       "5237976       196      480                540                       -1   \n",
       "5237977       197      480                540                        0   \n",
       "5237978       198      480                540                        1   \n",
       "5237979       199      480                540                       -1   \n",
       "\n",
       "         reference_price  matched_size  near_price  bid_price  ask_price  \\\n",
       "5237975         1.000317    28280362.0    0.999734   1.000317   1.000434   \n",
       "5237976         1.000643     9187699.0    1.000386   1.000643   1.000900   \n",
       "5237977         0.995789    12725436.0    0.995789   0.995789   0.995883   \n",
       "5237978         0.999210    94773272.0    0.999210   0.998970   0.999210   \n",
       "5237979         1.002129    24073678.0    1.001494   1.002129   1.002447   \n",
       "\n",
       "              wap  ...  ask_wap_diff  imbalance_buy_sell_flag_cumsum  \\\n",
       "5237975  1.000328  ...      0.000106                             -54   \n",
       "5237976  1.000819  ...      0.000081                             -55   \n",
       "5237977  0.995797  ...      0.000086                             -54   \n",
       "5237978  0.999008  ...      0.000202                              28   \n",
       "5237979  1.002274  ...      0.000173                             -35   \n",
       "\n",
       "        all_prices_mean  all_sizes_mean  all_prices_std  all_sizes_std  \\\n",
       "5237975        0.830386      7070091.00        0.416020     14140180.0   \n",
       "5237976        0.847850      2296925.50        0.374351      4593849.0   \n",
       "5237977        0.705909      3181358.25        0.710108      6362718.5   \n",
       "5237978        0.809469     23693318.00        0.464552     47386636.0   \n",
       "5237979        0.882249      6018420.50        0.293562     12036838.0   \n",
       "\n",
       "         all_prices_skew  all_sizes_skew  all_prices_kurt  all_sizes_kurt  \n",
       "5237975        -2.449488             2.0         5.999992             4.0  \n",
       "5237976        -2.449489             2.0         5.999995             4.0  \n",
       "5237977        -2.449490             2.0         6.000000             4.0  \n",
       "5237978        -2.449490             2.0         5.999999             4.0  \n",
       "5237979        -2.449483             2.0         5.999975             4.0  \n",
       "\n",
       "[5 rows x 43 columns]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def calc_feature_single_stock(df):\n",
    "    # Feature of each stock at different time point\n",
    "    # date_id\n",
    "    df[\"date_id_week\"] = df[\"date_id\"] % 5  # Day of the week\n",
    "    df[\"date_id_seconds\"] = df[\"seconds_in_bucket\"] % 60  # Seconds\n",
    "   \n",
    "    \n",
    "    # ask bid \n",
    "    df['spread'] = df['ask_price'] - df['bid_price']\n",
    "   \n",
    "    df['mid_price'] = (df['bid_price'] + df['ask_price'])/2\n",
    "    # product of imbalance size and bid-ask spread\n",
    "    df[\"price_pressure\"] = df[\"Quantile_imbalance_size\"] * (df[\"spread\"])\n",
    "    \n",
    "    # near_price far_price\n",
    "    df['near_far_ratio'] = df['near_price'] / df['Quantile_far_price']\n",
    "    df['near_far_imbalance'] = (df['Quantile_far_price'] - df['near_price']) / (df['Quantile_far_price'] + df['near_price'])\n",
    "   \n",
    "    \n",
    "    #The ratio between ask/bid and matched sizes provides insights into market liquidity and order flow:\n",
    "    # A high ask-to-matched ratio suggests that there are more sellers than buyers at a given price level, \n",
    "    # potentially indicating resistance to upward price movement.\n",
    "    # Conversely, a low ratio implies stronger buying interest and potential support for price increases.\n",
    "    \n",
    "    df['ask_mat_ratio'] = df['Quantile_ask_size']/df['matched_size']\n",
    "    df['bid_mat_ratio'] = df['Quantile_bid_size']/df['matched_size']\n",
    "    \n",
    "    # The ratio between the ask/bid price and the reference price provides insights into market sentiment:\n",
    "    # - Above 1: If the ask/bid price is higher than the reference price, it suggests bullish sentiment. Sellers are demanding a premium.\n",
    "    # - Below 1: If the ask/bid price is lower than the reference price, it indicates bearish sentiment. Buyers are getting a discount.\n",
    "    df['ask_ref_ratio'] = df['ask_price']/df['reference_price']\n",
    "    # imbalance_size matched_size\n",
    "    df['auction_volume'] = df['Quantile_imbalance_size'] + df['matched_size']\n",
    "    # relative imbalance between matched and total size\n",
    "    df[\"imbalance_ratio\"] = (df[\"Quantile_imbalance_size\"] - df[\"matched_size\"]) / (df[\"matched_size\"] + df[\"Quantile_imbalance_size\"])\n",
    "    # cross term \n",
    "    df[\"imbalance_continuous_ratio\"] = df['Quantile_imbalance_size'] / df[\"volume\"]\n",
    "    # the overall level of buying and selling in financial markets\n",
    "    df['market_activity'] = df['Quantile_bid_size'] * df['bid_price'] + df['Quantile_ask_size'] * df['ask_price']\n",
    "    # The difference between the ask/bid price and the WAP can indicate market efficiency.\n",
    "    df['ask_wap_diff'] = df['ask_price'] - df['wap']\n",
    "    # Accumulative features\n",
    "    df['imbalance_buy_sell_flag_cumsum'] =  df.groupby(['stock_id','date_id'])['imbalance_buy_sell_flag'].cumsum()\n",
    "    \n",
    "    # statistical features at different time point\n",
    "    prices = [\"reference_price\", \"Quantile_far_price\", \"near_price\", \"ask_price\", \"bid_price\", \"wap\"]\n",
    "    sizes = [\"matched_size\", \"Quantile_bid_size\", \"Quantile_ask_size\", \"Quantile_imbalance_size\"]\n",
    "    for func in [\"mean\", \"std\", \"skew\", \"kurt\"]:\n",
    "            df[f\"all_prices_{func}\"] = df[prices].agg(func, axis=1)\n",
    "            df[f\"all_sizes_{func}\"] = df[sizes].agg(func, axis=1)\n",
    "            \n",
    "\n",
    "    return df\n",
    "\n",
    "df_train = calc_feature_single_stock(df_train)\n",
    "df_train.tail()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stock_id</th>\n",
       "      <th>date_id</th>\n",
       "      <th>seconds_in_bucket</th>\n",
       "      <th>imbalance_buy_sell_flag</th>\n",
       "      <th>reference_price</th>\n",
       "      <th>matched_size</th>\n",
       "      <th>near_price</th>\n",
       "      <th>bid_price</th>\n",
       "      <th>ask_price</th>\n",
       "      <th>wap</th>\n",
       "      <th>...</th>\n",
       "      <th>imbalance_buy_sell_flag_cumsum</th>\n",
       "      <th>all_prices_mean</th>\n",
       "      <th>all_sizes_mean</th>\n",
       "      <th>all_prices_std</th>\n",
       "      <th>all_sizes_std</th>\n",
       "      <th>all_prices_skew</th>\n",
       "      <th>all_sizes_skew</th>\n",
       "      <th>all_prices_kurt</th>\n",
       "      <th>all_sizes_kurt</th>\n",
       "      <th>vwap_reference_price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999812</td>\n",
       "      <td>13380277.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.999812</td>\n",
       "      <td>1.000026</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999913</td>\n",
       "      <td>3.345070e+06</td>\n",
       "      <td>0.000117</td>\n",
       "      <td>6.690138e+06</td>\n",
       "      <td>0.042897</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9.589178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.999896</td>\n",
       "      <td>1642214.25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.999896</td>\n",
       "      <td>1.000660</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>1.000113</td>\n",
       "      <td>4.105531e+05</td>\n",
       "      <td>0.000368</td>\n",
       "      <td>8.211074e+05</td>\n",
       "      <td>1.897504</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.617163</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-1.921946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.999561</td>\n",
       "      <td>1819368.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.999403</td>\n",
       "      <td>1.000298</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.999816</td>\n",
       "      <td>4.548419e+05</td>\n",
       "      <td>0.000409</td>\n",
       "      <td>9.096841e+05</td>\n",
       "      <td>0.311287</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-2.953897</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-3.830244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>1.000171</td>\n",
       "      <td>18389746.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>1.000214</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>1.000096</td>\n",
       "      <td>4.597437e+06</td>\n",
       "      <td>0.000113</td>\n",
       "      <td>9.194873e+06</td>\n",
       "      <td>0.124467</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9.279469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.999532</td>\n",
       "      <td>17860614.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.999394</td>\n",
       "      <td>1.000016</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.999735</td>\n",
       "      <td>4.465153e+06</td>\n",
       "      <td>0.000320</td>\n",
       "      <td>8.930307e+06</td>\n",
       "      <td>-0.156620</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-5.078881</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-3.027896</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 44 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   stock_id  date_id  seconds_in_bucket  imbalance_buy_sell_flag  \\\n",
       "0         0        0                  0                        1   \n",
       "1         1        0                  0                       -1   \n",
       "2         2        0                  0                       -1   \n",
       "3         3        0                  0                       -1   \n",
       "4         4        0                  0                       -1   \n",
       "\n",
       "   reference_price  matched_size  near_price  bid_price  ask_price  wap  ...  \\\n",
       "0         0.999812   13380277.00         NaN   0.999812   1.000026  1.0  ...   \n",
       "1         0.999896    1642214.25         NaN   0.999896   1.000660  1.0  ...   \n",
       "2         0.999561    1819368.00         NaN   0.999403   1.000298  1.0  ...   \n",
       "3         1.000171   18389746.00         NaN   0.999999   1.000214  1.0  ...   \n",
       "4         0.999532   17860614.00         NaN   0.999394   1.000016  1.0  ...   \n",
       "\n",
       "   imbalance_buy_sell_flag_cumsum  all_prices_mean all_sizes_mean  \\\n",
       "0                               1         0.999913   3.345070e+06   \n",
       "1                              -1         1.000113   4.105531e+05   \n",
       "2                              -1         0.999816   4.548419e+05   \n",
       "3                              -1         1.000096   4.597437e+06   \n",
       "4                              -1         0.999735   4.465153e+06   \n",
       "\n",
       "   all_prices_std  all_sizes_std  all_prices_skew  all_sizes_skew  \\\n",
       "0        0.000117   6.690138e+06         0.042897             2.0   \n",
       "1        0.000368   8.211074e+05         1.897504             2.0   \n",
       "2        0.000409   9.096841e+05         0.311287             2.0   \n",
       "3        0.000113   9.194873e+06         0.124467             2.0   \n",
       "4        0.000320   8.930307e+06        -0.156620             2.0   \n",
       "\n",
       "   all_prices_kurt  all_sizes_kurt  vwap_reference_price  \n",
       "0         0.000000             4.0              9.589178  \n",
       "1         3.617163             4.0             -1.921946  \n",
       "2        -2.953897             4.0             -3.830244  \n",
       "3         0.000000             4.0              9.279469  \n",
       "4        -5.078881             4.0             -3.027896  \n",
       "\n",
       "[5 rows x 44 columns]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculate_vwap(df):\n",
    "    df['vwap_reference_price'] = (df['wap'] * df['reference_price']).cumsum() / df['volume'].cumsum()\n",
    "    return df\n",
    "df_train = calculate_vwap(df_train)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['rolling_std_reference_price'] = df_train.groupby('stock_id')['reference_price'].transform(lambda x: x.rolling(window=54).std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make dummy from imbalance_buy_sell_flag and get dummy columns\n",
    "dummy_cols_list = ['imbalance_buy_sell_flag']\n",
    "dummy_names = []\n",
    "\n",
    "for col in dummy_cols_list:\n",
    "    dummy_df = pd.get_dummies(df_train.loc[:, col], prefix=col, drop_first=True)\n",
    "    dummy_names += dummy_df.columns.tolist()\n",
    "    # drop original column\n",
    "    df_train = df_train.drop(col, axis=1)\n",
    "    df_train = pd.concat([df_train, dummy_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# drop these columns\n",
    "df_train = df_train.drop(['all_sizes_std','all_prices_std','all_sizes_mean','market_activity','mid_price'], axis=1)\n",
    "# drop: near_far_mid, auction_volume,all_prices_mean\n",
    "df_train = df_train.drop([ 'auction_volume','all_prices_mean'], axis=1)\n",
    "\n",
    "\n",
    "# drop: time_id\n",
    "df_train = df_train.drop(['time_id'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of feature selection (trading_at_the_close_visualisation_1.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = ['stock_id','target','imbalance_buy_sell_flag_0','imbalance_buy_sell_flag_1', 'rolling_std_reference_price','date_id','seconds_in_bucket', 'ask_ref_ratio', 'Quantile_bid_size', 'Quantile_ask_size',  'ask_wap_diff', 'spread']\n",
    "\n",
    "# keep only the selected features\n",
    "df_train = df_train[selected_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stock_id</th>\n",
       "      <th>target</th>\n",
       "      <th>imbalance_buy_sell_flag_0</th>\n",
       "      <th>imbalance_buy_sell_flag_1</th>\n",
       "      <th>rolling_std_reference_price</th>\n",
       "      <th>date_id</th>\n",
       "      <th>seconds_in_bucket</th>\n",
       "      <th>ask_ref_ratio</th>\n",
       "      <th>Quantile_bid_size</th>\n",
       "      <th>Quantile_ask_size</th>\n",
       "      <th>ask_wap_diff</th>\n",
       "      <th>spread</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-3.029704</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000214</td>\n",
       "      <td>0.739395</td>\n",
       "      <td>-0.635130</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.000214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>-5.519986</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000764</td>\n",
       "      <td>-1.068202</td>\n",
       "      <td>-0.076523</td>\n",
       "      <td>0.000660</td>\n",
       "      <td>0.000764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>-8.389950</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000737</td>\n",
       "      <td>0.391064</td>\n",
       "      <td>-0.133653</td>\n",
       "      <td>0.000298</td>\n",
       "      <td>0.000895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>-4.010201</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000043</td>\n",
       "      <td>-1.200443</td>\n",
       "      <td>2.414491</td>\n",
       "      <td>0.000214</td>\n",
       "      <td>0.000215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>-7.349849</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000484</td>\n",
       "      <td>-0.196919</td>\n",
       "      <td>-1.885052</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5237975</th>\n",
       "      <td>195</td>\n",
       "      <td>2.310276</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.000270</td>\n",
       "      <td>480</td>\n",
       "      <td>540</td>\n",
       "      <td>1.000117</td>\n",
       "      <td>0.266118</td>\n",
       "      <td>2.022891</td>\n",
       "      <td>0.000106</td>\n",
       "      <td>0.000117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5237976</th>\n",
       "      <td>196</td>\n",
       "      <td>-8.220077</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.000518</td>\n",
       "      <td>480</td>\n",
       "      <td>540</td>\n",
       "      <td>1.000257</td>\n",
       "      <td>1.651024</td>\n",
       "      <td>1.058055</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>0.000257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5237977</th>\n",
       "      <td>197</td>\n",
       "      <td>1.169443</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.001755</td>\n",
       "      <td>480</td>\n",
       "      <td>540</td>\n",
       "      <td>1.000094</td>\n",
       "      <td>-0.185484</td>\n",
       "      <td>1.539343</td>\n",
       "      <td>0.000086</td>\n",
       "      <td>0.000094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5237978</th>\n",
       "      <td>198</td>\n",
       "      <td>-1.540184</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0.000460</td>\n",
       "      <td>480</td>\n",
       "      <td>540</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.279195</td>\n",
       "      <td>2.784187</td>\n",
       "      <td>0.000202</td>\n",
       "      <td>0.000240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5237979</th>\n",
       "      <td>199</td>\n",
       "      <td>-6.530285</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.000485</td>\n",
       "      <td>480</td>\n",
       "      <td>540</td>\n",
       "      <td>1.000317</td>\n",
       "      <td>1.797340</td>\n",
       "      <td>1.965054</td>\n",
       "      <td>0.000173</td>\n",
       "      <td>0.000318</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5237760 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         stock_id    target  imbalance_buy_sell_flag_0  \\\n",
       "0               0 -3.029704                      False   \n",
       "1               1 -5.519986                      False   \n",
       "2               2 -8.389950                      False   \n",
       "3               3 -4.010201                      False   \n",
       "4               4 -7.349849                      False   \n",
       "...           ...       ...                        ...   \n",
       "5237975       195  2.310276                      False   \n",
       "5237976       196 -8.220077                      False   \n",
       "5237977       197  1.169443                       True   \n",
       "5237978       198 -1.540184                      False   \n",
       "5237979       199 -6.530285                      False   \n",
       "\n",
       "         imbalance_buy_sell_flag_1  rolling_std_reference_price  date_id  \\\n",
       "0                             True                          NaN        0   \n",
       "1                            False                          NaN        0   \n",
       "2                            False                          NaN        0   \n",
       "3                            False                          NaN        0   \n",
       "4                            False                          NaN        0   \n",
       "...                            ...                          ...      ...   \n",
       "5237975                      False                     0.000270      480   \n",
       "5237976                      False                     0.000518      480   \n",
       "5237977                      False                     0.001755      480   \n",
       "5237978                       True                     0.000460      480   \n",
       "5237979                      False                     0.000485      480   \n",
       "\n",
       "         seconds_in_bucket  ask_ref_ratio  Quantile_bid_size  \\\n",
       "0                        0       1.000214           0.739395   \n",
       "1                        0       1.000764          -1.068202   \n",
       "2                        0       1.000737           0.391064   \n",
       "3                        0       1.000043          -1.200443   \n",
       "4                        0       1.000484          -0.196919   \n",
       "...                    ...            ...                ...   \n",
       "5237975                540       1.000117           0.266118   \n",
       "5237976                540       1.000257           1.651024   \n",
       "5237977                540       1.000094          -0.185484   \n",
       "5237978                540       1.000000           1.279195   \n",
       "5237979                540       1.000317           1.797340   \n",
       "\n",
       "         Quantile_ask_size  ask_wap_diff    spread  \n",
       "0                -0.635130      0.000026  0.000214  \n",
       "1                -0.076523      0.000660  0.000764  \n",
       "2                -0.133653      0.000298  0.000895  \n",
       "3                 2.414491      0.000214  0.000215  \n",
       "4                -1.885052      0.000016  0.000622  \n",
       "...                    ...           ...       ...  \n",
       "5237975           2.022891      0.000106  0.000117  \n",
       "5237976           1.058055      0.000081  0.000257  \n",
       "5237977           1.539343      0.000086  0.000094  \n",
       "5237978           2.784187      0.000202  0.000240  \n",
       "5237979           1.965054      0.000173  0.000318  \n",
       "\n",
       "[5237760 rows x 12 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load from preprocessed file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load preprocess data\n",
    "df_train = pd.read_csv('data/optiver-trading-at-the-close/train_preprocess_short.csv')\n",
    "\n",
    "selected_features = ['stock_id','target','imbalance_buy_sell_flag_0','imbalance_buy_sell_flag_1', 'rolling_std_matched_size','date_id','seconds_in_bucket', 'ask_ref_ratio', 'Quantile_bid_size', 'Quantile_ask_size',  'ask_wap_diff', 'spread']\n",
    "\n",
    "# keep only the selected features\n",
    "df_train = df_train[selected_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.impute import KNNImputer\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random as python_random\n",
    "from keras import Sequential\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.layers import Input, Dense, LSTM, Dropout\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "from xgboost import XGBRegressor, plot_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "N_LAGS = 55\n",
    "BATCH_SIZE = 32\n",
    "BUFFER_SIZE = 100000\n",
    "EPOCHS = 10\n",
    "PATIENCE = 25\n",
    "DROPOUT = 0.5\n",
    "LEARNING_RATE = 1e-4\n",
    "SPLIT_DAY = 390\n",
    "N_STOCKS = 200\n",
    "N_DATES = 481\n",
    "N_SECONDS = 55\n",
    "RUN_TRAINING = True\n",
    "RUN_FOR_SUBMISSION = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DATA FORMATTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def windowed_dataset(dataset, shuffle=True):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(dataset)\n",
    "    dataset = dataset.window(N_LAGS + 1, shift=1, drop_remainder=True)\n",
    "    dataset = dataset.flat_map(lambda window: window.batch(N_LAGS + 1))\n",
    "    dataset = dataset.map(lambda window: (window[:-1], window[-1]))\n",
    "    if shuffle:\n",
    "      dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "    dataset = dataset.batch(BATCH_SIZE).prefetch(1)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def build_features(df):\n",
    "\n",
    "    all_stock_ids = range(N_STOCKS)\n",
    "    all_date_ids = df[\"date_id\"].unique()\n",
    "    all_seconds = [i * 10 for i in range(N_SECONDS)]\n",
    "\n",
    "    multi_index = pd.MultiIndex.from_product([all_stock_ids, all_date_ids, all_seconds],\n",
    "                                             names=['stock_id', 'date_id', 'seconds_in_bucket'])\n",
    "    df_full = df.set_index(['stock_id', 'date_id', 'seconds_in_bucket']).reindex(multi_index)\n",
    "    df_full = df_full.fillna(0)\n",
    "    df_full = df_full.reset_index()\n",
    "\n",
    "    df_pivoted = df_full.pivot_table(\n",
    "                values='target',\n",
    "                index=['date_id', 'seconds_in_bucket'],\n",
    "                columns='stock_id')\n",
    "\n",
    "    df_pivoted = df_pivoted.reset_index(drop=True)\n",
    "    df_pivoted.columns.name = None\n",
    "\n",
    "    return df_pivoted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "build_features(df_train)\n",
    "\n",
    "def build_model(dropout=DROPOUT):\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(N_LAGS, N_STOCKS)))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(LSTM(25, return_sequences=False))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(N_STOCKS))\n",
    "    model.compile(loss='mae',\n",
    "                  optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\User\\Alkalmazások\\python\\lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:From c:\\Users\\User\\Alkalmazások\\python\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "671/671 [==============================] - 20s 25ms/step - loss: 0.7371 - val_loss: 0.6856\n",
      "Epoch 2/10\n",
      "671/671 [==============================] - 18s 25ms/step - loss: 0.7279 - val_loss: 0.6799\n",
      "Epoch 3/10\n",
      "671/671 [==============================] - 19s 26ms/step - loss: 0.7206 - val_loss: 0.6749\n",
      "Epoch 4/10\n",
      "671/671 [==============================] - 19s 26ms/step - loss: 0.7146 - val_loss: 0.6716\n",
      "Epoch 5/10\n",
      "671/671 [==============================] - 20s 27ms/step - loss: 0.7104 - val_loss: 0.6693\n",
      "Epoch 6/10\n",
      "671/671 [==============================] - 21s 28ms/step - loss: 0.7075 - val_loss: 0.6674\n",
      "Epoch 7/10\n",
      "671/671 [==============================] - 20s 27ms/step - loss: 0.7052 - val_loss: 0.6657\n",
      "Epoch 8/10\n",
      "671/671 [==============================] - 20s 27ms/step - loss: 0.7032 - val_loss: 0.6641\n",
      "Epoch 9/10\n",
      "671/671 [==============================] - 20s 27ms/step - loss: 0.7017 - val_loss: 0.6627\n",
      "Epoch 10/10\n",
      "671/671 [==============================] - 20s 27ms/step - loss: 0.7002 - val_loss: 0.6615\n",
      "153/153 [==============================] - 1s 4ms/step\n",
      "MAE score: 5.871362209320068\n"
     ]
    }
   ],
   "source": [
    "if RUN_TRAINING:\n",
    "\n",
    "  split = df_train['date_id'] > SPLIT_DAY\n",
    "  df_train_ = df_train[~split]\n",
    "  df_valid = df_train[split]\n",
    "\n",
    "  df_train_features = build_features(df_train_)\n",
    "  df_valid_features = build_features(df_valid)\n",
    "\n",
    "  scaler = StandardScaler()\n",
    "  train_features = scaler.fit_transform(df_train_features)\n",
    "  valid_features = scaler.transform(df_valid_features)\n",
    "\n",
    "  train_dataset = windowed_dataset(train_features)\n",
    "  valid_dataset = windowed_dataset(valid_features, shuffle=False)\n",
    "\n",
    "  model = build_model()\n",
    "\n",
    "  early_stopping = EarlyStopping(monitor='val_loss',\n",
    "                    mode='min',\n",
    "                    patience=PATIENCE,\n",
    "                    restore_best_weights=True,\n",
    "                    verbose=True)\n",
    "\n",
    "  history = model.fit(train_dataset,\n",
    "                      validation_data=valid_dataset,\n",
    "                      epochs=EPOCHS,\n",
    "                      batch_size=BATCH_SIZE,\n",
    "                      callbacks=[early_stopping],\n",
    "                      verbose=True)\n",
    "\n",
    "  ## Evaluate ##\n",
    "  y_pred = model.predict(valid_dataset)\n",
    "\n",
    "  y_pred = scaler.inverse_transform(y_pred)\n",
    "  y_true = df_valid_features[N_LAGS:]\n",
    "\n",
    "  mae = mean_absolute_error(y_true, y_pred)\n",
    "  print(f\"MAE score: {mae}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Plots ##\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model train vs validation loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "671/671 [==============================] - 23s 29ms/step - loss: 0.7314 - val_loss: 0.6845\n",
      "Epoch 2/10\n",
      "671/671 [==============================] - 20s 27ms/step - loss: 0.7214 - val_loss: 0.6774\n",
      "Epoch 3/10\n",
      "671/671 [==============================] - 21s 29ms/step - loss: 0.7134 - val_loss: 0.6725\n",
      "Epoch 4/10\n",
      "671/671 [==============================] - 39s 55ms/step - loss: 0.7078 - val_loss: 0.6692\n",
      "Epoch 5/10\n",
      "671/671 [==============================] - 25s 32ms/step - loss: 0.7036 - val_loss: 0.6664\n",
      "Epoch 6/10\n",
      "671/671 [==============================] - 22s 30ms/step - loss: 0.7003 - val_loss: 0.6639\n",
      "Epoch 7/10\n",
      "671/671 [==============================] - 22s 29ms/step - loss: 0.6971 - val_loss: 0.6617\n",
      "Epoch 8/10\n",
      "671/671 [==============================] - 22s 29ms/step - loss: 0.6947 - val_loss: 0.6599\n",
      "Epoch 9/10\n",
      "671/671 [==============================] - 22s 29ms/step - loss: 0.6929 - val_loss: 0.6583\n",
      "Epoch 10/10\n",
      "671/671 [==============================] - 22s 30ms/step - loss: 0.6911 - val_loss: 0.6570\n",
      "153/153 [==============================] - 1s 5ms/step\n",
      "MAE score for hyperparameters {'N_LAGS': 55, 'BATCH_SIZE': 32, 'DROPOUT': 0.3, 'LEARNING_RATE': 0.0001, 'LSTM_UNITS': 25}: 5.835030555725098\n",
      "Epoch 1/10\n",
      "671/671 [==============================] - 32s 42ms/step - loss: 0.7373 - val_loss: 0.6848\n",
      "Epoch 2/10\n",
      "671/671 [==============================] - 30s 42ms/step - loss: 0.7200 - val_loss: 0.6742\n",
      "Epoch 3/10\n",
      "671/671 [==============================] - 49s 71ms/step - loss: 0.7090 - val_loss: 0.6676\n",
      "Epoch 4/10\n",
      "671/671 [==============================] - 52s 74ms/step - loss: 0.7014 - val_loss: 0.6625\n",
      "Epoch 5/10\n",
      "671/671 [==============================] - 35s 48ms/step - loss: 0.6955 - val_loss: 0.6583\n",
      "Epoch 6/10\n",
      "671/671 [==============================] - 34s 48ms/step - loss: 0.6906 - val_loss: 0.6547\n",
      "Epoch 7/10\n",
      "671/671 [==============================] - 103s 149ms/step - loss: 0.6869 - val_loss: 0.6516\n",
      "Epoch 8/10\n",
      "671/671 [==============================] - 36s 51ms/step - loss: 0.6833 - val_loss: 0.6489\n",
      "Epoch 9/10\n",
      "671/671 [==============================] - 47s 66ms/step - loss: 0.6808 - val_loss: 0.6465\n",
      "Epoch 10/10\n",
      "671/671 [==============================] - 34s 47ms/step - loss: 0.6785 - val_loss: 0.6443\n",
      "153/153 [==============================] - 1s 6ms/step\n",
      "MAE score for hyperparameters {'N_LAGS': 55, 'BATCH_SIZE': 32, 'DROPOUT': 0.3, 'LEARNING_RATE': 0.0001, 'LSTM_UNITS': 50}: 5.729333877563477\n",
      "Epoch 1/10\n",
      "671/671 [==============================] - 25s 31ms/step - loss: 0.7066 - val_loss: 0.6570\n",
      "Epoch 2/10\n",
      "671/671 [==============================] - 58s 84ms/step - loss: 0.6875 - val_loss: 0.6498\n",
      "Epoch 3/10\n",
      "671/671 [==============================] - 27s 38ms/step - loss: 0.6827 - val_loss: 0.6462\n",
      "Epoch 4/10\n",
      "671/671 [==============================] - 41s 56ms/step - loss: 0.6804 - val_loss: 0.6442\n",
      "Epoch 5/10\n",
      "671/671 [==============================] - 41s 56ms/step - loss: 0.6784 - val_loss: 0.6427\n",
      "Epoch 6/10\n",
      "671/671 [==============================] - 40s 55ms/step - loss: 0.6776 - val_loss: 0.6416\n",
      "Epoch 7/10\n",
      "671/671 [==============================] - 40s 55ms/step - loss: 0.6764 - val_loss: 0.6408\n",
      "Epoch 8/10\n",
      "671/671 [==============================] - 23s 31ms/step - loss: 0.6758 - val_loss: 0.6403\n",
      "Epoch 9/10\n",
      "671/671 [==============================] - 21s 29ms/step - loss: 0.6754 - val_loss: 0.6394\n",
      "Epoch 10/10\n",
      "671/671 [==============================] - 21s 28ms/step - loss: 0.6747 - val_loss: 0.6391\n",
      "153/153 [==============================] - 1s 4ms/step\n",
      "MAE score for hyperparameters {'N_LAGS': 55, 'BATCH_SIZE': 32, 'DROPOUT': 0.3, 'LEARNING_RATE': 0.001, 'LSTM_UNITS': 25}: 5.696233749389648\n",
      "Epoch 1/10\n",
      "671/671 [==============================] - 32s 43ms/step - loss: 0.7011 - val_loss: 0.6470\n",
      "Epoch 2/10\n",
      "671/671 [==============================] - 31s 43ms/step - loss: 0.6756 - val_loss: 0.6353\n",
      "Epoch 3/10\n",
      "671/671 [==============================] - 30s 42ms/step - loss: 0.6691 - val_loss: 0.6305\n",
      "Epoch 4/10\n",
      "671/671 [==============================] - 30s 42ms/step - loss: 0.6659 - val_loss: 0.6278\n",
      "Epoch 5/10\n",
      "671/671 [==============================] - 30s 43ms/step - loss: 0.6638 - val_loss: 0.6260\n",
      "Epoch 6/10\n",
      "671/671 [==============================] - 30s 42ms/step - loss: 0.6624 - val_loss: 0.6249\n",
      "Epoch 7/10\n",
      "671/671 [==============================] - 30s 42ms/step - loss: 0.6614 - val_loss: 0.6242\n",
      "Epoch 8/10\n",
      "671/671 [==============================] - 31s 44ms/step - loss: 0.6604 - val_loss: 0.6235\n",
      "Epoch 9/10\n",
      "671/671 [==============================] - 32s 44ms/step - loss: 0.6596 - val_loss: 0.6231\n",
      "Epoch 10/10\n",
      "671/671 [==============================] - 32s 45ms/step - loss: 0.6594 - val_loss: 0.6223\n",
      "153/153 [==============================] - 1s 6ms/step\n",
      "MAE score for hyperparameters {'N_LAGS': 55, 'BATCH_SIZE': 32, 'DROPOUT': 0.3, 'LEARNING_RATE': 0.001, 'LSTM_UNITS': 50}: 5.550782680511475\n",
      "Epoch 1/10\n",
      "671/671 [==============================] - 27s 34ms/step - loss: 0.7372 - val_loss: 0.6858\n",
      "Epoch 2/10\n",
      "671/671 [==============================] - 24s 32ms/step - loss: 0.7282 - val_loss: 0.6803\n",
      "Epoch 3/10\n",
      "671/671 [==============================] - 23s 31ms/step - loss: 0.7210 - val_loss: 0.6756\n",
      "Epoch 4/10\n",
      "671/671 [==============================] - 23s 31ms/step - loss: 0.7152 - val_loss: 0.6721\n",
      "Epoch 5/10\n",
      "671/671 [==============================] - 23s 32ms/step - loss: 0.7110 - val_loss: 0.6697\n",
      "Epoch 6/10\n",
      "671/671 [==============================] - 24s 33ms/step - loss: 0.7080 - val_loss: 0.6677\n",
      "Epoch 7/10\n",
      "671/671 [==============================] - 23s 32ms/step - loss: 0.7056 - val_loss: 0.6659\n",
      "Epoch 8/10\n",
      "671/671 [==============================] - 23s 32ms/step - loss: 0.7034 - val_loss: 0.6643\n",
      "Epoch 9/10\n",
      "671/671 [==============================] - 24s 33ms/step - loss: 0.7017 - val_loss: 0.6628\n",
      "Epoch 10/10\n",
      "671/671 [==============================] - 23s 31ms/step - loss: 0.7001 - val_loss: 0.6615\n",
      "153/153 [==============================] - 1s 4ms/step\n",
      "MAE score for hyperparameters {'N_LAGS': 55, 'BATCH_SIZE': 32, 'DROPOUT': 0.5, 'LEARNING_RATE': 0.0001, 'LSTM_UNITS': 25}: 5.872622013092041\n",
      "Epoch 1/10\n",
      "671/671 [==============================] - 34s 47ms/step - loss: 0.7469 - val_loss: 0.6878\n",
      "Epoch 2/10\n",
      "671/671 [==============================] - 32s 45ms/step - loss: 0.7313 - val_loss: 0.6782\n",
      "Epoch 3/10\n",
      "671/671 [==============================] - 33s 46ms/step - loss: 0.7198 - val_loss: 0.6716\n",
      "Epoch 4/10\n",
      "671/671 [==============================] - 32s 45ms/step - loss: 0.7121 - val_loss: 0.6672\n",
      "Epoch 5/10\n",
      "671/671 [==============================] - 33s 46ms/step - loss: 0.7063 - val_loss: 0.6639\n",
      "Epoch 6/10\n",
      "671/671 [==============================] - 32s 45ms/step - loss: 0.7025 - val_loss: 0.6610\n",
      "Epoch 7/10\n",
      "671/671 [==============================] - 32s 45ms/step - loss: 0.6990 - val_loss: 0.6586\n",
      "Epoch 8/10\n",
      "671/671 [==============================] - 32s 45ms/step - loss: 0.6965 - val_loss: 0.6565\n",
      "Epoch 9/10\n",
      "671/671 [==============================] - 32s 45ms/step - loss: 0.6944 - val_loss: 0.6548\n",
      "Epoch 10/10\n",
      "671/671 [==============================] - 34s 48ms/step - loss: 0.6924 - val_loss: 0.6533\n",
      "153/153 [==============================] - 1s 6ms/step\n",
      "MAE score for hyperparameters {'N_LAGS': 55, 'BATCH_SIZE': 32, 'DROPOUT': 0.5, 'LEARNING_RATE': 0.0001, 'LSTM_UNITS': 50}: 5.803797721862793\n",
      "Epoch 1/10\n",
      "671/671 [==============================] - 23s 30ms/step - loss: 0.7146 - val_loss: 0.6631\n",
      "Epoch 2/10\n",
      "671/671 [==============================] - 22s 30ms/step - loss: 0.6980 - val_loss: 0.6571\n",
      "Epoch 3/10\n",
      "671/671 [==============================] - 21s 29ms/step - loss: 0.6946 - val_loss: 0.6548\n",
      "Epoch 4/10\n",
      "671/671 [==============================] - 22s 30ms/step - loss: 0.6931 - val_loss: 0.6534\n",
      "Epoch 5/10\n",
      "671/671 [==============================] - 22s 30ms/step - loss: 0.6921 - val_loss: 0.6526\n",
      "Epoch 6/10\n",
      "671/671 [==============================] - 22s 30ms/step - loss: 0.6917 - val_loss: 0.6521\n",
      "Epoch 7/10\n",
      "671/671 [==============================] - 22s 30ms/step - loss: 0.6909 - val_loss: 0.6514\n",
      "Epoch 8/10\n",
      "671/671 [==============================] - 21s 29ms/step - loss: 0.6901 - val_loss: 0.6507\n",
      "Epoch 9/10\n",
      "671/671 [==============================] - 22s 30ms/step - loss: 0.6899 - val_loss: 0.6507\n",
      "Epoch 10/10\n",
      "671/671 [==============================] - 23s 31ms/step - loss: 0.6895 - val_loss: 0.6504\n",
      "153/153 [==============================] - 1s 5ms/step\n",
      "MAE score for hyperparameters {'N_LAGS': 55, 'BATCH_SIZE': 32, 'DROPOUT': 0.5, 'LEARNING_RATE': 0.001, 'LSTM_UNITS': 25}: 5.786030292510986\n",
      "Epoch 1/10\n",
      "671/671 [==============================] - 33s 45ms/step - loss: 0.7118 - val_loss: 0.6547\n",
      "Epoch 2/10\n",
      "671/671 [==============================] - 14476s 22s/step - loss: 0.6907 - val_loss: 0.6475\n",
      "Epoch 3/10\n",
      "671/671 [==============================] - 41s 55ms/step - loss: 0.6864 - val_loss: 0.6444\n",
      "Epoch 4/10\n",
      "671/671 [==============================] - 36s 50ms/step - loss: 0.6846 - val_loss: 0.6428\n",
      "Epoch 5/10\n",
      "671/671 [==============================] - 34s 47ms/step - loss: 0.6835 - val_loss: 0.6416\n",
      "Epoch 6/10\n",
      "671/671 [==============================] - 15142s 23s/step - loss: 0.6822 - val_loss: 0.6406\n",
      "Epoch 7/10\n",
      "671/671 [==============================] - 10598s 16s/step - loss: 0.6817 - val_loss: 0.6405\n",
      "Epoch 8/10\n",
      "671/671 [==============================] - 36s 50ms/step - loss: 0.6816 - val_loss: 0.6398\n",
      "Epoch 9/10\n",
      "671/671 [==============================] - 32s 44ms/step - loss: 0.6807 - val_loss: 0.6391\n",
      "Epoch 10/10\n",
      "671/671 [==============================] - 39s 54ms/step - loss: 0.6806 - val_loss: 0.6390\n",
      "153/153 [==============================] - 2s 9ms/step\n",
      "MAE score for hyperparameters {'N_LAGS': 55, 'BATCH_SIZE': 32, 'DROPOUT': 0.5, 'LEARNING_RATE': 0.001, 'LSTM_UNITS': 50}: 5.69157600402832\n",
      "Epoch 1/10\n",
      "671/671 [==============================] - 43s 58ms/step - loss: 0.7496 - val_loss: 0.6869\n",
      "Epoch 2/10\n",
      "671/671 [==============================] - 40s 57ms/step - loss: 0.7374 - val_loss: 0.6828\n",
      "Epoch 3/10\n",
      "671/671 [==============================] - 41s 57ms/step - loss: 0.7296 - val_loss: 0.6799\n",
      "Epoch 4/10\n",
      "671/671 [==============================] - 40s 56ms/step - loss: 0.7245 - val_loss: 0.6775\n",
      "Epoch 5/10\n",
      "671/671 [==============================] - 40s 56ms/step - loss: 0.7203 - val_loss: 0.6751\n",
      "Epoch 6/10\n",
      "671/671 [==============================] - 33s 45ms/step - loss: 0.7171 - val_loss: 0.6731\n",
      "Epoch 7/10\n",
      "671/671 [==============================] - 24s 33ms/step - loss: 0.7145 - val_loss: 0.6716\n",
      "Epoch 8/10\n",
      "671/671 [==============================] - 29s 41ms/step - loss: 0.7127 - val_loss: 0.6706\n",
      "Epoch 9/10\n",
      "671/671 [==============================] - 35s 49ms/step - loss: 0.7112 - val_loss: 0.6698\n",
      "Epoch 10/10\n",
      "671/671 [==============================] - 40s 57ms/step - loss: 0.7103 - val_loss: 0.6690\n",
      "153/153 [==============================] - 1s 6ms/step\n",
      "MAE score for hyperparameters {'N_LAGS': 55, 'BATCH_SIZE': 32, 'DROPOUT': 0.7, 'LEARNING_RATE': 0.0001, 'LSTM_UNITS': 25}: 5.930789947509766\n",
      "Epoch 1/10\n",
      "671/671 [==============================] - 52s 71ms/step - loss: 0.7677 - val_loss: 0.6894\n",
      "Epoch 2/10\n",
      "671/671 [==============================] - 56s 79ms/step - loss: 0.7479 - val_loss: 0.6819\n",
      "Epoch 3/10\n",
      "671/671 [==============================] - 56s 80ms/step - loss: 0.7344 - val_loss: 0.6763\n",
      "Epoch 4/10\n",
      "671/671 [==============================] - 37s 51ms/step - loss: 0.7252 - val_loss: 0.6724\n",
      "Epoch 5/10\n",
      "671/671 [==============================] - 31s 44ms/step - loss: 0.7185 - val_loss: 0.6698\n",
      "Epoch 6/10\n",
      "671/671 [==============================] - 32s 46ms/step - loss: 0.7139 - val_loss: 0.6680\n",
      "Epoch 7/10\n",
      "671/671 [==============================] - 31s 45ms/step - loss: 0.7108 - val_loss: 0.6665\n",
      "Epoch 8/10\n",
      "671/671 [==============================] - 32s 45ms/step - loss: 0.7085 - val_loss: 0.6651\n",
      "Epoch 9/10\n",
      "671/671 [==============================] - 32s 45ms/step - loss: 0.7066 - val_loss: 0.6639\n",
      "Epoch 10/10\n",
      "671/671 [==============================] - 31s 45ms/step - loss: 0.7051 - val_loss: 0.6627\n",
      "153/153 [==============================] - 1s 4ms/step\n",
      "MAE score for hyperparameters {'N_LAGS': 55, 'BATCH_SIZE': 32, 'DROPOUT': 0.7, 'LEARNING_RATE': 0.0001, 'LSTM_UNITS': 50}: 5.881045341491699\n",
      "Epoch 1/10\n",
      "671/671 [==============================] - 23s 31ms/step - loss: 0.7233 - val_loss: 0.6696\n",
      "Epoch 2/10\n",
      "671/671 [==============================] - 22s 30ms/step - loss: 0.7078 - val_loss: 0.6646\n",
      "Epoch 3/10\n",
      "671/671 [==============================] - 21s 29ms/step - loss: 0.7053 - val_loss: 0.6627\n",
      "Epoch 4/10\n",
      "671/671 [==============================] - 22s 30ms/step - loss: 0.7042 - val_loss: 0.6617\n",
      "Epoch 5/10\n",
      "671/671 [==============================] - 29s 40ms/step - loss: 0.7035 - val_loss: 0.6614\n",
      "Epoch 6/10\n",
      "671/671 [==============================] - 27s 37ms/step - loss: 0.7029 - val_loss: 0.6611\n",
      "Epoch 7/10\n",
      "671/671 [==============================] - 25s 35ms/step - loss: 0.7024 - val_loss: 0.6608\n",
      "Epoch 8/10\n",
      "671/671 [==============================] - 21s 29ms/step - loss: 0.7023 - val_loss: 0.6605\n",
      "Epoch 9/10\n",
      "671/671 [==============================] - 21s 28ms/step - loss: 0.7021 - val_loss: 0.6602\n",
      "Epoch 10/10\n",
      "671/671 [==============================] - 21s 29ms/step - loss: 0.7018 - val_loss: 0.6602\n",
      "153/153 [==============================] - 1s 4ms/step\n",
      "MAE score for hyperparameters {'N_LAGS': 55, 'BATCH_SIZE': 32, 'DROPOUT': 0.7, 'LEARNING_RATE': 0.001, 'LSTM_UNITS': 25}: 5.863569259643555\n",
      "Epoch 1/10\n",
      "671/671 [==============================] - 39s 52ms/step - loss: 0.7259 - val_loss: 0.6647\n",
      "Epoch 2/10\n",
      "671/671 [==============================] - 39s 55ms/step - loss: 0.7038 - val_loss: 0.6591\n",
      "Epoch 3/10\n",
      "671/671 [==============================] - 62s 88ms/step - loss: 0.7009 - val_loss: 0.6574\n",
      "Epoch 4/10\n",
      "671/671 [==============================] - 60s 84ms/step - loss: 0.6998 - val_loss: 0.6565\n",
      "Epoch 5/10\n",
      "671/671 [==============================] - 39s 54ms/step - loss: 0.6988 - val_loss: 0.6558\n",
      "Epoch 6/10\n",
      "671/671 [==============================] - 38s 52ms/step - loss: 0.6981 - val_loss: 0.6555\n",
      "Epoch 7/10\n",
      "671/671 [==============================] - 36s 51ms/step - loss: 0.6980 - val_loss: 0.6553\n",
      "Epoch 8/10\n",
      "671/671 [==============================] - 35s 50ms/step - loss: 0.6975 - val_loss: 0.6551\n",
      "Epoch 9/10\n",
      "671/671 [==============================] - 36s 51ms/step - loss: 0.6976 - val_loss: 0.6550\n",
      "Epoch 10/10\n",
      "671/671 [==============================] - 35s 50ms/step - loss: 0.6972 - val_loss: 0.6551\n",
      "153/153 [==============================] - 1s 6ms/step\n",
      "MAE score for hyperparameters {'N_LAGS': 55, 'BATCH_SIZE': 32, 'DROPOUT': 0.7, 'LEARNING_RATE': 0.001, 'LSTM_UNITS': 50}: 5.822524547576904\n",
      "Epoch 1/10\n",
      "671/671 [==============================] - 26s 34ms/step - loss: 0.7319 - val_loss: 0.6850\n",
      "Epoch 2/10\n",
      "671/671 [==============================] - 24s 33ms/step - loss: 0.7218 - val_loss: 0.6778\n",
      "Epoch 3/10\n",
      "671/671 [==============================] - 24s 33ms/step - loss: 0.7135 - val_loss: 0.6728\n",
      "Epoch 4/10\n",
      "671/671 [==============================] - 24s 34ms/step - loss: 0.7076 - val_loss: 0.6695\n",
      "Epoch 5/10\n",
      "671/671 [==============================] - 24s 33ms/step - loss: 0.7033 - val_loss: 0.6667\n",
      "Epoch 6/10\n",
      "671/671 [==============================] - 24s 33ms/step - loss: 0.7000 - val_loss: 0.6642\n",
      "Epoch 7/10\n",
      "671/671 [==============================] - 25s 34ms/step - loss: 0.6972 - val_loss: 0.6620\n",
      "Epoch 8/10\n",
      "671/671 [==============================] - 24s 34ms/step - loss: 0.6950 - val_loss: 0.6602\n",
      "Epoch 9/10\n",
      "671/671 [==============================] - 24s 34ms/step - loss: 0.6929 - val_loss: 0.6586\n",
      "Epoch 10/10\n",
      "671/671 [==============================] - 24s 34ms/step - loss: 0.6911 - val_loss: 0.6573\n",
      "153/153 [==============================] - 1s 4ms/step\n",
      "MAE score for hyperparameters {'N_LAGS': 55, 'BATCH_SIZE': 64, 'DROPOUT': 0.3, 'LEARNING_RATE': 0.0001, 'LSTM_UNITS': 25}: 5.8365631103515625\n",
      "Epoch 1/10\n",
      "671/671 [==============================] - 36s 49ms/step - loss: 0.7375 - val_loss: 0.6861\n",
      "Epoch 2/10\n",
      "671/671 [==============================] - 35s 49ms/step - loss: 0.7207 - val_loss: 0.6753\n",
      "Epoch 3/10\n",
      "671/671 [==============================] - 35s 49ms/step - loss: 0.7097 - val_loss: 0.6686\n",
      "Epoch 4/10\n",
      "671/671 [==============================] - 35s 49ms/step - loss: 0.7017 - val_loss: 0.6633\n",
      "Epoch 5/10\n",
      "671/671 [==============================] - 34s 48ms/step - loss: 0.6956 - val_loss: 0.6589\n",
      "Epoch 6/10\n",
      "671/671 [==============================] - 34s 48ms/step - loss: 0.6907 - val_loss: 0.6552\n",
      "Epoch 7/10\n",
      "671/671 [==============================] - 34s 49ms/step - loss: 0.6868 - val_loss: 0.6520\n",
      "Epoch 8/10\n",
      "671/671 [==============================] - 35s 49ms/step - loss: 0.6835 - val_loss: 0.6492\n",
      "Epoch 9/10\n",
      "671/671 [==============================] - 42s 60ms/step - loss: 0.6806 - val_loss: 0.6468\n",
      "Epoch 10/10\n",
      "671/671 [==============================] - 34s 48ms/step - loss: 0.6785 - val_loss: 0.6446\n",
      "153/153 [==============================] - 1s 7ms/step\n",
      "MAE score for hyperparameters {'N_LAGS': 55, 'BATCH_SIZE': 64, 'DROPOUT': 0.3, 'LEARNING_RATE': 0.0001, 'LSTM_UNITS': 50}: 5.732229232788086\n",
      "Epoch 1/10\n",
      "671/671 [==============================] - 24s 31ms/step - loss: 0.7068 - val_loss: 0.6578\n",
      "Epoch 2/10\n",
      "671/671 [==============================] - 24s 33ms/step - loss: 0.6881 - val_loss: 0.6503\n",
      "Epoch 3/10\n",
      "671/671 [==============================] - 23s 31ms/step - loss: 0.6832 - val_loss: 0.6469\n",
      "Epoch 4/10\n",
      "671/671 [==============================] - 23s 32ms/step - loss: 0.6806 - val_loss: 0.6448\n",
      "Epoch 5/10\n",
      "671/671 [==============================] - 35s 49ms/step - loss: 0.6788 - val_loss: 0.6430\n",
      "Epoch 6/10\n",
      "671/671 [==============================] - 43s 60ms/step - loss: 0.6776 - val_loss: 0.6417\n",
      "Epoch 7/10\n",
      "671/671 [==============================] - 42s 58ms/step - loss: 0.6764 - val_loss: 0.6412\n",
      "Epoch 8/10\n",
      "671/671 [==============================] - 41s 58ms/step - loss: 0.6757 - val_loss: 0.6399\n",
      "Epoch 9/10\n",
      "671/671 [==============================] - 42s 57ms/step - loss: 0.6750 - val_loss: 0.6394\n",
      "Epoch 10/10\n",
      "671/671 [==============================] - 41s 57ms/step - loss: 0.6745 - val_loss: 0.6387\n",
      "153/153 [==============================] - 1s 5ms/step\n",
      "MAE score for hyperparameters {'N_LAGS': 55, 'BATCH_SIZE': 64, 'DROPOUT': 0.3, 'LEARNING_RATE': 0.001, 'LSTM_UNITS': 25}: 5.693472385406494\n",
      "Epoch 1/10\n",
      "671/671 [==============================] - 55s 77ms/step - loss: 0.7011 - val_loss: 0.6474\n",
      "Epoch 2/10\n",
      "671/671 [==============================] - 51s 73ms/step - loss: 0.6759 - val_loss: 0.6356\n",
      "Epoch 3/10\n",
      "671/671 [==============================] - 61s 87ms/step - loss: 0.6691 - val_loss: 0.6310\n",
      "Epoch 4/10\n",
      "671/671 [==============================] - 59s 84ms/step - loss: 0.6663 - val_loss: 0.6283\n",
      "Epoch 5/10\n",
      "671/671 [==============================] - 54s 76ms/step - loss: 0.6639 - val_loss: 0.6264\n",
      "Epoch 6/10\n",
      "671/671 [==============================] - 34s 48ms/step - loss: 0.6625 - val_loss: 0.6248\n",
      "Epoch 7/10\n",
      "671/671 [==============================] - 34s 48ms/step - loss: 0.6614 - val_loss: 0.6241\n",
      "Epoch 8/10\n",
      "671/671 [==============================] - 33s 46ms/step - loss: 0.6604 - val_loss: 0.6237\n",
      "Epoch 9/10\n",
      "671/671 [==============================] - 32s 45ms/step - loss: 0.6600 - val_loss: 0.6228\n",
      "Epoch 10/10\n",
      "671/671 [==============================] - 33s 47ms/step - loss: 0.6594 - val_loss: 0.6224\n",
      "153/153 [==============================] - 1s 6ms/step\n",
      "MAE score for hyperparameters {'N_LAGS': 55, 'BATCH_SIZE': 64, 'DROPOUT': 0.3, 'LEARNING_RATE': 0.001, 'LSTM_UNITS': 50}: 5.552180290222168\n",
      "Epoch 1/10\n",
      "671/671 [==============================] - 23s 30ms/step - loss: 0.7367 - val_loss: 0.6845\n",
      "Epoch 2/10\n",
      "671/671 [==============================] - 23s 32ms/step - loss: 0.7272 - val_loss: 0.6789\n",
      "Epoch 3/10\n",
      "671/671 [==============================] - 23s 31ms/step - loss: 0.7199 - val_loss: 0.6746\n",
      "Epoch 4/10\n",
      "671/671 [==============================] - 22s 30ms/step - loss: 0.7145 - val_loss: 0.6717\n",
      "Epoch 5/10\n",
      "671/671 [==============================] - 23s 31ms/step - loss: 0.7107 - val_loss: 0.6696\n",
      "Epoch 6/10\n",
      "671/671 [==============================] - 23s 31ms/step - loss: 0.7080 - val_loss: 0.6679\n",
      "Epoch 7/10\n",
      "671/671 [==============================] - 22s 31ms/step - loss: 0.7057 - val_loss: 0.6661\n",
      "Epoch 8/10\n",
      "671/671 [==============================] - 23s 31ms/step - loss: 0.7036 - val_loss: 0.6646\n",
      "Epoch 9/10\n",
      "671/671 [==============================] - 23s 31ms/step - loss: 0.7018 - val_loss: 0.6632\n",
      "Epoch 10/10\n",
      "671/671 [==============================] - 22s 30ms/step - loss: 0.7007 - val_loss: 0.6620\n",
      "153/153 [==============================] - 1s 4ms/step\n",
      "MAE score for hyperparameters {'N_LAGS': 55, 'BATCH_SIZE': 64, 'DROPOUT': 0.5, 'LEARNING_RATE': 0.0001, 'LSTM_UNITS': 25}: 5.875500679016113\n",
      "Epoch 1/10\n",
      "671/671 [==============================] - 33s 45ms/step - loss: 0.7464 - val_loss: 0.6873\n",
      "Epoch 2/10\n",
      "671/671 [==============================] - 32s 45ms/step - loss: 0.7305 - val_loss: 0.6775\n",
      "Epoch 3/10\n",
      "671/671 [==============================] - 32s 45ms/step - loss: 0.7193 - val_loss: 0.6711\n",
      "Epoch 4/10\n",
      "671/671 [==============================] - 32s 45ms/step - loss: 0.7116 - val_loss: 0.6668\n",
      "Epoch 5/10\n",
      "671/671 [==============================] - 32s 45ms/step - loss: 0.7061 - val_loss: 0.6634\n",
      "Epoch 6/10\n",
      "671/671 [==============================] - 32s 46ms/step - loss: 0.7023 - val_loss: 0.6605\n",
      "Epoch 7/10\n",
      "671/671 [==============================] - 32s 45ms/step - loss: 0.6991 - val_loss: 0.6581\n",
      "Epoch 8/10\n",
      "671/671 [==============================] - 32s 46ms/step - loss: 0.6963 - val_loss: 0.6561\n",
      "Epoch 9/10\n",
      "671/671 [==============================] - 37s 52ms/step - loss: 0.6942 - val_loss: 0.6545\n",
      "Epoch 10/10\n",
      "671/671 [==============================] - 60s 85ms/step - loss: 0.6927 - val_loss: 0.6530\n",
      "153/153 [==============================] - 2s 10ms/step\n",
      "MAE score for hyperparameters {'N_LAGS': 55, 'BATCH_SIZE': 64, 'DROPOUT': 0.5, 'LEARNING_RATE': 0.0001, 'LSTM_UNITS': 50}: 5.8006768226623535\n",
      "Epoch 1/10\n",
      "671/671 [==============================] - 44s 59ms/step - loss: 0.7143 - val_loss: 0.6628\n",
      "Epoch 2/10\n",
      "671/671 [==============================] - 42s 58ms/step - loss: 0.6978 - val_loss: 0.6568\n",
      "Epoch 3/10\n",
      "671/671 [==============================] - 44s 61ms/step - loss: 0.6943 - val_loss: 0.6548\n",
      "Epoch 4/10\n",
      "671/671 [==============================] - 43s 60ms/step - loss: 0.6929 - val_loss: 0.6537\n",
      "Epoch 5/10\n",
      "671/671 [==============================] - 43s 60ms/step - loss: 0.6917 - val_loss: 0.6528\n",
      "Epoch 6/10\n",
      "671/671 [==============================] - 42s 58ms/step - loss: 0.6912 - val_loss: 0.6522\n",
      "Epoch 7/10\n",
      "671/671 [==============================] - 42s 58ms/step - loss: 0.6906 - val_loss: 0.6518\n",
      "Epoch 8/10\n",
      "671/671 [==============================] - 44s 61ms/step - loss: 0.6901 - val_loss: 0.6509\n",
      "Epoch 9/10\n",
      "671/671 [==============================] - 43s 60ms/step - loss: 0.6898 - val_loss: 0.6509\n",
      "Epoch 10/10\n",
      "671/671 [==============================] - 42s 59ms/step - loss: 0.6895 - val_loss: 0.6507\n",
      "153/153 [==============================] - 1s 6ms/step\n",
      "MAE score for hyperparameters {'N_LAGS': 55, 'BATCH_SIZE': 64, 'DROPOUT': 0.5, 'LEARNING_RATE': 0.001, 'LSTM_UNITS': 25}: 5.787563323974609\n",
      "Epoch 1/10\n",
      "671/671 [==============================] - 67s 91ms/step - loss: 0.7118 - val_loss: 0.6546\n",
      "Epoch 2/10\n",
      "671/671 [==============================] - 65s 92ms/step - loss: 0.6901 - val_loss: 0.6477\n",
      "Epoch 3/10\n",
      "671/671 [==============================] - 65s 91ms/step - loss: 0.6867 - val_loss: 0.6444\n",
      "Epoch 4/10\n",
      "671/671 [==============================] - 65s 91ms/step - loss: 0.6845 - val_loss: 0.6432\n",
      "Epoch 5/10\n",
      "671/671 [==============================] - 64s 90ms/step - loss: 0.6834 - val_loss: 0.6419\n",
      "Epoch 6/10\n",
      "671/671 [==============================] - 56s 78ms/step - loss: 0.6825 - val_loss: 0.6410\n",
      "Epoch 7/10\n",
      "671/671 [==============================] - 46s 64ms/step - loss: 0.6815 - val_loss: 0.6405\n",
      "Epoch 8/10\n",
      "671/671 [==============================] - 52s 73ms/step - loss: 0.6811 - val_loss: 0.6397\n",
      "Epoch 9/10\n",
      "671/671 [==============================] - 63s 88ms/step - loss: 0.6809 - val_loss: 0.6397\n",
      "Epoch 10/10\n",
      "671/671 [==============================] - 61s 86ms/step - loss: 0.6809 - val_loss: 0.6393\n",
      "153/153 [==============================] - 2s 11ms/step\n",
      "MAE score for hyperparameters {'N_LAGS': 55, 'BATCH_SIZE': 64, 'DROPOUT': 0.5, 'LEARNING_RATE': 0.001, 'LSTM_UNITS': 50}: 5.694301128387451\n",
      "Epoch 1/10\n",
      "671/671 [==============================] - 42s 57ms/step - loss: 0.7504 - val_loss: 0.6870\n",
      "Epoch 2/10\n",
      "671/671 [==============================] - 44s 61ms/step - loss: 0.7382 - val_loss: 0.6829\n",
      "Epoch 3/10\n",
      "671/671 [==============================] - 43s 60ms/step - loss: 0.7304 - val_loss: 0.6799\n",
      "Epoch 4/10\n",
      "671/671 [==============================] - 44s 60ms/step - loss: 0.7247 - val_loss: 0.6773\n",
      "Epoch 5/10\n",
      "671/671 [==============================] - 44s 62ms/step - loss: 0.7206 - val_loss: 0.6750\n",
      "Epoch 6/10\n",
      "671/671 [==============================] - 43s 60ms/step - loss: 0.7172 - val_loss: 0.6731\n",
      "Epoch 7/10\n",
      "671/671 [==============================] - 32s 42ms/step - loss: 0.7146 - val_loss: 0.6717\n",
      "Epoch 8/10\n",
      "671/671 [==============================] - 25s 34ms/step - loss: 0.7129 - val_loss: 0.6707\n",
      "Epoch 9/10\n",
      "671/671 [==============================] - 26s 36ms/step - loss: 0.7116 - val_loss: 0.6700\n",
      "Epoch 10/10\n",
      "671/671 [==============================] - 31s 42ms/step - loss: 0.7105 - val_loss: 0.6693\n",
      "153/153 [==============================] - 1s 5ms/step\n",
      "MAE score for hyperparameters {'N_LAGS': 55, 'BATCH_SIZE': 64, 'DROPOUT': 0.7, 'LEARNING_RATE': 0.0001, 'LSTM_UNITS': 25}: 5.933004856109619\n",
      "Epoch 1/10\n",
      "671/671 [==============================] - 38s 52ms/step - loss: 0.7669 - val_loss: 0.6896\n",
      "Epoch 2/10\n",
      "671/671 [==============================] - 34s 48ms/step - loss: 0.7475 - val_loss: 0.6823\n",
      "Epoch 3/10\n",
      "671/671 [==============================] - 35s 50ms/step - loss: 0.7343 - val_loss: 0.6768\n",
      "Epoch 4/10\n",
      "671/671 [==============================] - 48s 69ms/step - loss: 0.7251 - val_loss: 0.6727\n",
      "Epoch 5/10\n",
      "671/671 [==============================] - 51s 72ms/step - loss: 0.7187 - val_loss: 0.6700\n",
      "Epoch 6/10\n",
      "671/671 [==============================] - 38s 53ms/step - loss: 0.7141 - val_loss: 0.6681\n",
      "Epoch 7/10\n",
      "671/671 [==============================] - 36s 50ms/step - loss: 0.7111 - val_loss: 0.6666\n",
      "Epoch 8/10\n",
      "671/671 [==============================] - 36s 51ms/step - loss: 0.7088 - val_loss: 0.6652\n",
      "Epoch 9/10\n",
      "671/671 [==============================] - 35s 50ms/step - loss: 0.7067 - val_loss: 0.6640\n",
      "Epoch 10/10\n",
      "671/671 [==============================] - 37s 52ms/step - loss: 0.7053 - val_loss: 0.6628\n",
      "153/153 [==============================] - 2s 8ms/step\n",
      "MAE score for hyperparameters {'N_LAGS': 55, 'BATCH_SIZE': 64, 'DROPOUT': 0.7, 'LEARNING_RATE': 0.0001, 'LSTM_UNITS': 50}: 5.881452560424805\n",
      "Epoch 1/10\n",
      "671/671 [==============================] - 26s 34ms/step - loss: 0.7235 - val_loss: 0.6696\n",
      "Epoch 2/10\n",
      "671/671 [==============================] - 25s 35ms/step - loss: 0.7082 - val_loss: 0.6646\n",
      "Epoch 3/10\n",
      "671/671 [==============================] - 25s 35ms/step - loss: 0.7055 - val_loss: 0.6626\n",
      "Epoch 4/10\n",
      "671/671 [==============================] - 25s 35ms/step - loss: 0.7042 - val_loss: 0.6616\n",
      "Epoch 5/10\n",
      "671/671 [==============================] - 34s 48ms/step - loss: 0.7035 - val_loss: 0.6612\n",
      "Epoch 6/10\n",
      "671/671 [==============================] - 35s 47ms/step - loss: 0.7031 - val_loss: 0.6608\n",
      "Epoch 7/10\n",
      "671/671 [==============================] - 39s 52ms/step - loss: 0.7026 - val_loss: 0.6604\n",
      "Epoch 8/10\n",
      "671/671 [==============================] - 25s 35ms/step - loss: 0.7020 - val_loss: 0.6601\n",
      "Epoch 9/10\n",
      "671/671 [==============================] - 25s 35ms/step - loss: 0.7020 - val_loss: 0.6600\n",
      "Epoch 10/10\n",
      "671/671 [==============================] - 27s 37ms/step - loss: 0.7016 - val_loss: 0.6601\n",
      "153/153 [==============================] - 1s 5ms/step\n",
      "MAE score for hyperparameters {'N_LAGS': 55, 'BATCH_SIZE': 64, 'DROPOUT': 0.7, 'LEARNING_RATE': 0.001, 'LSTM_UNITS': 25}: 5.862817287445068\n",
      "Epoch 1/10\n",
      "671/671 [==============================] - 41s 56ms/step - loss: 0.7258 - val_loss: 0.6650\n",
      "Epoch 2/10\n",
      "671/671 [==============================] - 39s 53ms/step - loss: 0.7041 - val_loss: 0.6589\n",
      "Epoch 3/10\n",
      "671/671 [==============================] - 41s 57ms/step - loss: 0.7009 - val_loss: 0.6569\n",
      "Epoch 4/10\n",
      "671/671 [==============================] - 38s 54ms/step - loss: 0.6998 - val_loss: 0.6563\n",
      "Epoch 5/10\n",
      "671/671 [==============================] - 37s 53ms/step - loss: 0.6987 - val_loss: 0.6555\n",
      "Epoch 6/10\n",
      "671/671 [==============================] - 43s 59ms/step - loss: 0.6983 - val_loss: 0.6554\n",
      "Epoch 7/10\n",
      "671/671 [==============================] - 40s 57ms/step - loss: 0.6975 - val_loss: 0.6551\n",
      "Epoch 8/10\n",
      "671/671 [==============================] - 42s 58ms/step - loss: 0.6977 - val_loss: 0.6549\n",
      "Epoch 9/10\n",
      "671/671 [==============================] - 40s 56ms/step - loss: 0.6971 - val_loss: 0.6548\n",
      "Epoch 10/10\n",
      "671/671 [==============================] - 42s 58ms/step - loss: 0.6973 - val_loss: 0.6547\n",
      "153/153 [==============================] - 2s 10ms/step\n",
      "MAE score for hyperparameters {'N_LAGS': 55, 'BATCH_SIZE': 64, 'DROPOUT': 0.7, 'LEARNING_RATE': 0.001, 'LSTM_UNITS': 50}: 5.820108413696289\n",
      "Epoch 1/10\n",
      "671/671 [==============================] - 31s 40ms/step - loss: 0.7310 - val_loss: 0.6839\n",
      "Epoch 2/10\n",
      "671/671 [==============================] - 30s 40ms/step - loss: 0.7206 - val_loss: 0.6766\n",
      "Epoch 3/10\n",
      "671/671 [==============================] - 29s 40ms/step - loss: 0.7127 - val_loss: 0.6720\n",
      "Epoch 4/10\n",
      "671/671 [==============================] - 30s 40ms/step - loss: 0.7074 - val_loss: 0.6689\n",
      "Epoch 5/10\n",
      "671/671 [==============================] - 30s 40ms/step - loss: 0.7036 - val_loss: 0.6662\n",
      "Epoch 6/10\n",
      "671/671 [==============================] - 29s 40ms/step - loss: 0.7001 - val_loss: 0.6637\n",
      "Epoch 7/10\n",
      "671/671 [==============================] - 935s 1s/step - loss: 0.6974 - val_loss: 0.6615\n",
      "Epoch 8/10\n",
      "671/671 [==============================] - 35s 47ms/step - loss: 0.6949 - val_loss: 0.6595\n",
      "Epoch 9/10\n",
      "671/671 [==============================] - 32s 43ms/step - loss: 0.6928 - val_loss: 0.6579\n",
      "Epoch 10/10\n",
      "671/671 [==============================] - 34s 48ms/step - loss: 0.6913 - val_loss: 0.6566\n",
      "153/153 [==============================] - 1s 6ms/step\n",
      "MAE score for hyperparameters {'N_LAGS': 60, 'BATCH_SIZE': 32, 'DROPOUT': 0.3, 'LEARNING_RATE': 0.0001, 'LSTM_UNITS': 25}: 5.832203388214111\n",
      "Epoch 1/10\n",
      "671/671 [==============================] - 50s 68ms/step - loss: 0.7369 - val_loss: 0.6857\n",
      "Epoch 2/10\n",
      "671/671 [==============================] - 55s 79ms/step - loss: 0.7199 - val_loss: 0.6750\n",
      "Epoch 3/10\n",
      "671/671 [==============================] - 49s 69ms/step - loss: 0.7092 - val_loss: 0.6683\n",
      "Epoch 4/10\n",
      "671/671 [==============================] - 49s 68ms/step - loss: 0.7017 - val_loss: 0.6631\n",
      "Epoch 5/10\n",
      "671/671 [==============================] - 44s 62ms/step - loss: 0.6960 - val_loss: 0.6588\n",
      "Epoch 6/10\n",
      "671/671 [==============================] - 50s 71ms/step - loss: 0.6912 - val_loss: 0.6552\n",
      "Epoch 7/10\n",
      "671/671 [==============================] - 59s 85ms/step - loss: 0.6872 - val_loss: 0.6522\n",
      "Epoch 8/10\n",
      "671/671 [==============================] - 45s 62ms/step - loss: 0.6839 - val_loss: 0.6495\n",
      "Epoch 9/10\n",
      "671/671 [==============================] - 37s 53ms/step - loss: 0.6814 - val_loss: 0.6472\n",
      "Epoch 10/10\n",
      "671/671 [==============================] - 36s 51ms/step - loss: 0.6791 - val_loss: 0.6451\n",
      "153/153 [==============================] - 1s 6ms/step\n",
      "MAE score for hyperparameters {'N_LAGS': 60, 'BATCH_SIZE': 32, 'DROPOUT': 0.3, 'LEARNING_RATE': 0.0001, 'LSTM_UNITS': 50}: 5.735811710357666\n",
      "Epoch 1/10\n",
      "671/671 [==============================] - 25s 33ms/step - loss: 0.7071 - val_loss: 0.6579\n",
      "Epoch 2/10\n",
      "671/671 [==============================] - 26s 37ms/step - loss: 0.6880 - val_loss: 0.6502\n",
      "Epoch 3/10\n",
      "671/671 [==============================] - 26s 35ms/step - loss: 0.6832 - val_loss: 0.6467\n",
      "Epoch 4/10\n",
      "671/671 [==============================] - 29s 39ms/step - loss: 0.6806 - val_loss: 0.6445\n",
      "Epoch 5/10\n",
      "671/671 [==============================] - 25s 34ms/step - loss: 0.6788 - val_loss: 0.6429\n",
      "Epoch 6/10\n",
      "671/671 [==============================] - 25s 35ms/step - loss: 0.6777 - val_loss: 0.6421\n",
      "Epoch 7/10\n",
      "671/671 [==============================] - 30s 41ms/step - loss: 0.6766 - val_loss: 0.6413\n",
      "Epoch 8/10\n",
      "671/671 [==============================] - 42s 58ms/step - loss: 0.6761 - val_loss: 0.6404\n",
      "Epoch 9/10\n",
      "671/671 [==============================] - 27s 36ms/step - loss: 0.6753 - val_loss: 0.6400\n",
      "Epoch 10/10\n",
      "671/671 [==============================] - 25s 34ms/step - loss: 0.6748 - val_loss: 0.6394\n",
      "153/153 [==============================] - 1s 4ms/step\n",
      "MAE score for hyperparameters {'N_LAGS': 60, 'BATCH_SIZE': 32, 'DROPOUT': 0.3, 'LEARNING_RATE': 0.001, 'LSTM_UNITS': 25}: 5.698251247406006\n",
      "Epoch 1/10\n",
      "671/671 [==============================] - 50s 69ms/step - loss: 0.7005 - val_loss: 0.6466\n",
      "Epoch 2/10\n",
      "671/671 [==============================] - 39s 55ms/step - loss: 0.6756 - val_loss: 0.6352\n",
      "Epoch 3/10\n",
      "671/671 [==============================] - 51s 73ms/step - loss: 0.6691 - val_loss: 0.6302\n",
      "Epoch 4/10\n",
      "671/671 [==============================] - 65s 93ms/step - loss: 0.6661 - val_loss: 0.6276\n",
      "Epoch 5/10\n",
      "671/671 [==============================] - 43s 60ms/step - loss: 0.6641 - val_loss: 0.6256\n",
      "Epoch 6/10\n",
      "671/671 [==============================] - 35s 49ms/step - loss: 0.6626 - val_loss: 0.6250\n",
      "Epoch 7/10\n",
      "671/671 [==============================] - 40s 57ms/step - loss: 0.6613 - val_loss: 0.6241\n",
      "Epoch 8/10\n",
      "671/671 [==============================] - 39s 55ms/step - loss: 0.6605 - val_loss: 0.6231\n",
      "Epoch 9/10\n",
      "671/671 [==============================] - 48s 68ms/step - loss: 0.6598 - val_loss: 0.6229\n",
      "Epoch 10/10\n",
      "671/671 [==============================] - 36s 50ms/step - loss: 0.6594 - val_loss: 0.6221\n",
      "153/153 [==============================] - 2s 8ms/step\n",
      "MAE score for hyperparameters {'N_LAGS': 60, 'BATCH_SIZE': 32, 'DROPOUT': 0.3, 'LEARNING_RATE': 0.001, 'LSTM_UNITS': 50}: 5.548815727233887\n",
      "Epoch 1/10\n",
      "671/671 [==============================] - 30s 39ms/step - loss: 0.7375 - val_loss: 0.6855\n",
      "Epoch 2/10\n",
      "671/671 [==============================] - 28s 39ms/step - loss: 0.7280 - val_loss: 0.6796\n",
      "Epoch 3/10\n",
      "671/671 [==============================] - 29s 40ms/step - loss: 0.7204 - val_loss: 0.6748\n",
      "Epoch 4/10\n",
      "671/671 [==============================] - 28s 39ms/step - loss: 0.7147 - val_loss: 0.6716\n",
      "Epoch 5/10\n",
      "671/671 [==============================] - 29s 39ms/step - loss: 0.7107 - val_loss: 0.6693\n",
      "Epoch 6/10\n",
      "671/671 [==============================] - 29s 40ms/step - loss: 0.7077 - val_loss: 0.6674\n",
      "Epoch 7/10\n",
      "671/671 [==============================] - 29s 39ms/step - loss: 0.7053 - val_loss: 0.6657\n",
      "Epoch 8/10\n",
      "671/671 [==============================] - 28s 39ms/step - loss: 0.7033 - val_loss: 0.6641\n",
      "Epoch 9/10\n",
      "671/671 [==============================] - 30s 41ms/step - loss: 0.7017 - val_loss: 0.6627\n",
      "Epoch 10/10\n",
      "671/671 [==============================] - 30s 41ms/step - loss: 0.7003 - val_loss: 0.6615\n",
      "153/153 [==============================] - 1s 7ms/step\n",
      "MAE score for hyperparameters {'N_LAGS': 60, 'BATCH_SIZE': 32, 'DROPOUT': 0.5, 'LEARNING_RATE': 0.0001, 'LSTM_UNITS': 25}: 5.87178897857666\n",
      "Epoch 1/10\n",
      "671/671 [==============================] - 41s 56ms/step - loss: 0.7469 - val_loss: 0.6871\n",
      "Epoch 2/10\n",
      "671/671 [==============================] - 39s 55ms/step - loss: 0.7310 - val_loss: 0.6777\n",
      "Epoch 3/10\n",
      "671/671 [==============================] - 39s 54ms/step - loss: 0.7196 - val_loss: 0.6715\n",
      "Epoch 4/10\n",
      "671/671 [==============================] - 38s 53ms/step - loss: 0.7120 - val_loss: 0.6672\n",
      "Epoch 5/10\n",
      "671/671 [==============================] - 39s 55ms/step - loss: 0.7067 - val_loss: 0.6639\n",
      "Epoch 6/10\n",
      "671/671 [==============================] - 39s 55ms/step - loss: 0.7024 - val_loss: 0.6611\n",
      "Epoch 7/10\n",
      "671/671 [==============================] - 39s 54ms/step - loss: 0.6993 - val_loss: 0.6587\n",
      "Epoch 8/10\n",
      "671/671 [==============================] - 51s 72ms/step - loss: 0.6966 - val_loss: 0.6567\n",
      "Epoch 9/10\n",
      "671/671 [==============================] - 48s 66ms/step - loss: 0.6944 - val_loss: 0.6550\n",
      "Epoch 10/10\n",
      "671/671 [==============================] - 41s 58ms/step - loss: 0.6927 - val_loss: 0.6534\n",
      "153/153 [==============================] - 2s 10ms/step\n",
      "MAE score for hyperparameters {'N_LAGS': 60, 'BATCH_SIZE': 32, 'DROPOUT': 0.5, 'LEARNING_RATE': 0.0001, 'LSTM_UNITS': 50}: 5.804769992828369\n",
      "Epoch 1/10\n",
      "671/671 [==============================] - 45s 62ms/step - loss: 0.7138 - val_loss: 0.6625\n",
      "Epoch 2/10\n",
      "671/671 [==============================] - 52s 70ms/step - loss: 0.6978 - val_loss: 0.6566\n",
      "Epoch 3/10\n",
      "671/671 [==============================] - 42s 56ms/step - loss: 0.6942 - val_loss: 0.6541\n",
      "Epoch 4/10\n",
      "671/671 [==============================] - 35s 49ms/step - loss: 0.6928 - val_loss: 0.6530\n",
      "Epoch 5/10\n",
      "671/671 [==============================] - 51s 70ms/step - loss: 0.6918 - val_loss: 0.6522\n",
      "Epoch 6/10\n",
      "671/671 [==============================] - 49s 67ms/step - loss: 0.6909 - val_loss: 0.6515\n",
      "Epoch 7/10\n",
      "671/671 [==============================] - 52s 71ms/step - loss: 0.6905 - val_loss: 0.6513\n",
      "Epoch 8/10\n",
      "671/671 [==============================] - 50s 68ms/step - loss: 0.6901 - val_loss: 0.6508\n",
      "Epoch 9/10\n",
      "671/671 [==============================] - 50s 68ms/step - loss: 0.6895 - val_loss: 0.6505\n",
      "Epoch 10/10\n",
      "671/671 [==============================] - 39s 52ms/step - loss: 0.6892 - val_loss: 0.6499\n",
      "153/153 [==============================] - 1s 5ms/step\n",
      "MAE score for hyperparameters {'N_LAGS': 60, 'BATCH_SIZE': 32, 'DROPOUT': 0.5, 'LEARNING_RATE': 0.001, 'LSTM_UNITS': 25}: 5.782882213592529\n",
      "Epoch 1/10\n",
      "671/671 [==============================] - 44s 61ms/step - loss: 0.7119 - val_loss: 0.6544\n",
      "Epoch 2/10\n",
      "671/671 [==============================] - 40s 55ms/step - loss: 0.6905 - val_loss: 0.6472\n",
      "Epoch 3/10\n",
      "671/671 [==============================] - 42s 60ms/step - loss: 0.6865 - val_loss: 0.6445\n",
      "Epoch 4/10\n",
      "671/671 [==============================] - 67s 93ms/step - loss: 0.6847 - val_loss: 0.6427\n",
      "Epoch 5/10\n",
      "671/671 [==============================] - 49s 67ms/step - loss: 0.6832 - val_loss: 0.6418\n",
      "Epoch 6/10\n",
      "671/671 [==============================] - 38s 54ms/step - loss: 0.6825 - val_loss: 0.6414\n",
      "Epoch 7/10\n",
      "671/671 [==============================] - 48s 69ms/step - loss: 0.6818 - val_loss: 0.6407\n",
      "Epoch 8/10\n",
      "671/671 [==============================] - 39s 55ms/step - loss: 0.6812 - val_loss: 0.6400\n",
      "Epoch 9/10\n",
      "671/671 [==============================] - 64s 90ms/step - loss: 0.6808 - val_loss: 0.6400\n",
      "Epoch 10/10\n",
      "671/671 [==============================] - 66s 92ms/step - loss: 0.6803 - val_loss: 0.6392\n",
      "153/153 [==============================] - 2s 11ms/step\n",
      "MAE score for hyperparameters {'N_LAGS': 60, 'BATCH_SIZE': 32, 'DROPOUT': 0.5, 'LEARNING_RATE': 0.001, 'LSTM_UNITS': 50}: 5.693288803100586\n",
      "Epoch 1/10\n",
      "671/671 [==============================] - 51s 66ms/step - loss: 0.7497 - val_loss: 0.6865\n",
      "Epoch 2/10\n",
      "671/671 [==============================] - 46s 64ms/step - loss: 0.7372 - val_loss: 0.6824\n",
      "Epoch 3/10\n",
      "671/671 [==============================] - 45s 62ms/step - loss: 0.7294 - val_loss: 0.6794\n",
      "Epoch 4/10\n",
      "671/671 [==============================] - 39s 53ms/step - loss: 0.7240 - val_loss: 0.6769\n",
      "Epoch 5/10\n",
      "671/671 [==============================] - 25s 35ms/step - loss: 0.7200 - val_loss: 0.6747\n",
      "Epoch 6/10\n",
      "671/671 [==============================] - 27s 37ms/step - loss: 0.7166 - val_loss: 0.6730\n",
      "Epoch 7/10\n",
      "671/671 [==============================] - 27s 38ms/step - loss: 0.7144 - val_loss: 0.6717\n",
      "Epoch 8/10\n",
      "671/671 [==============================] - 27s 37ms/step - loss: 0.7125 - val_loss: 0.6707\n",
      "Epoch 9/10\n",
      "671/671 [==============================] - 27s 37ms/step - loss: 0.7113 - val_loss: 0.6699\n",
      "Epoch 10/10\n",
      "671/671 [==============================] - 34s 46ms/step - loss: 0.7102 - val_loss: 0.6691\n",
      "153/153 [==============================] - 1s 6ms/step\n",
      "MAE score for hyperparameters {'N_LAGS': 60, 'BATCH_SIZE': 32, 'DROPOUT': 0.7, 'LEARNING_RATE': 0.0001, 'LSTM_UNITS': 25}: 5.931178092956543\n",
      "Epoch 1/10\n",
      "671/671 [==============================] - 41s 55ms/step - loss: 0.7680 - val_loss: 0.6894\n",
      "Epoch 2/10\n",
      "671/671 [==============================] - 47s 67ms/step - loss: 0.7482 - val_loss: 0.6819\n",
      "Epoch 3/10\n",
      "671/671 [==============================] - 47s 64ms/step - loss: 0.7348 - val_loss: 0.6765\n",
      "Epoch 4/10\n",
      "671/671 [==============================] - 59s 85ms/step - loss: 0.7252 - val_loss: 0.6726\n",
      "Epoch 5/10\n",
      "671/671 [==============================] - 47s 64ms/step - loss: 0.7188 - val_loss: 0.6700\n",
      "Epoch 6/10\n",
      "671/671 [==============================] - 67s 94ms/step - loss: 0.7141 - val_loss: 0.6682\n",
      "Epoch 7/10\n",
      "671/671 [==============================] - 42s 57ms/step - loss: 0.7111 - val_loss: 0.6667\n",
      "Epoch 8/10\n",
      "671/671 [==============================] - 39s 55ms/step - loss: 0.7089 - val_loss: 0.6654\n",
      "Epoch 9/10\n",
      "671/671 [==============================] - 39s 54ms/step - loss: 0.7070 - val_loss: 0.6641\n",
      "Epoch 10/10\n",
      "671/671 [==============================] - 38s 54ms/step - loss: 0.7053 - val_loss: 0.6630\n",
      "153/153 [==============================] - 2s 9ms/step\n",
      "MAE score for hyperparameters {'N_LAGS': 60, 'BATCH_SIZE': 32, 'DROPOUT': 0.7, 'LEARNING_RATE': 0.0001, 'LSTM_UNITS': 50}: 5.881971836090088\n",
      "Epoch 1/10\n",
      "671/671 [==============================] - 39s 53ms/step - loss: 0.7239 - val_loss: 0.6700\n",
      "Epoch 2/10\n",
      "671/671 [==============================] - 27s 37ms/step - loss: 0.7083 - val_loss: 0.6651\n",
      "Epoch 3/10\n",
      "671/671 [==============================] - 28s 38ms/step - loss: 0.7051 - val_loss: 0.6625\n",
      "Epoch 4/10\n",
      "671/671 [==============================] - 37s 53ms/step - loss: 0.7042 - val_loss: 0.6618\n",
      "Epoch 5/10\n",
      "671/671 [==============================] - 51s 70ms/step - loss: 0.7034 - val_loss: 0.6613\n",
      "Epoch 6/10\n",
      "671/671 [==============================] - 49s 67ms/step - loss: 0.7032 - val_loss: 0.6611\n",
      "Epoch 7/10\n",
      "671/671 [==============================] - 49s 67ms/step - loss: 0.7025 - val_loss: 0.6607\n",
      "Epoch 8/10\n",
      "671/671 [==============================] - 37s 51ms/step - loss: 0.7021 - val_loss: 0.6603\n",
      "Epoch 9/10\n",
      "671/671 [==============================] - 27s 37ms/step - loss: 0.7020 - val_loss: 0.6604\n",
      "Epoch 10/10\n",
      "671/671 [==============================] - 26s 36ms/step - loss: 0.7020 - val_loss: 0.6603\n",
      "153/153 [==============================] - 1s 6ms/step\n",
      "MAE score for hyperparameters {'N_LAGS': 60, 'BATCH_SIZE': 32, 'DROPOUT': 0.7, 'LEARNING_RATE': 0.001, 'LSTM_UNITS': 25}: 5.8640828132629395\n",
      "Epoch 1/10\n",
      "671/671 [==============================] - 39s 53ms/step - loss: 0.7258 - val_loss: 0.6650\n",
      "Epoch 2/10\n",
      "671/671 [==============================] - 37s 52ms/step - loss: 0.7040 - val_loss: 0.6592\n",
      "Epoch 3/10\n",
      "671/671 [==============================] - 37s 52ms/step - loss: 0.7009 - val_loss: 0.6573\n",
      "Epoch 4/10\n",
      "671/671 [==============================] - 39s 54ms/step - loss: 0.6994 - val_loss: 0.6562\n",
      "Epoch 5/10\n",
      "671/671 [==============================] - 52s 74ms/step - loss: 0.6987 - val_loss: 0.6558\n",
      "Epoch 6/10\n",
      "671/671 [==============================] - 42s 58ms/step - loss: 0.6982 - val_loss: 0.6553\n",
      "Epoch 7/10\n",
      "671/671 [==============================] - 40s 56ms/step - loss: 0.6980 - val_loss: 0.6554\n",
      "Epoch 8/10\n",
      "671/671 [==============================] - 41s 58ms/step - loss: 0.6976 - val_loss: 0.6547\n",
      "Epoch 9/10\n",
      "671/671 [==============================] - 41s 57ms/step - loss: 0.6974 - val_loss: 0.6547\n",
      "Epoch 10/10\n",
      "671/671 [==============================] - 42s 59ms/step - loss: 0.6969 - val_loss: 0.6544\n",
      "153/153 [==============================] - 2s 9ms/step\n",
      "MAE score for hyperparameters {'N_LAGS': 60, 'BATCH_SIZE': 32, 'DROPOUT': 0.7, 'LEARNING_RATE': 0.001, 'LSTM_UNITS': 50}: 5.817799091339111\n",
      "Epoch 1/10\n",
      "671/671 [==============================] - 30s 39ms/step - loss: 0.7308 - val_loss: 0.6841\n",
      "Epoch 2/10\n",
      "671/671 [==============================] - 27s 37ms/step - loss: 0.7206 - val_loss: 0.6770\n",
      "Epoch 3/10\n",
      "671/671 [==============================] - 27s 37ms/step - loss: 0.7125 - val_loss: 0.6723\n",
      "Epoch 4/10\n",
      "671/671 [==============================] - 26s 35ms/step - loss: 0.7071 - val_loss: 0.6688\n",
      "Epoch 5/10\n",
      "671/671 [==============================] - 26s 36ms/step - loss: 0.7028 - val_loss: 0.6658\n",
      "Epoch 6/10\n",
      "671/671 [==============================] - 26s 36ms/step - loss: 0.6995 - val_loss: 0.6632\n",
      "Epoch 7/10\n",
      "671/671 [==============================] - 26s 35ms/step - loss: 0.6966 - val_loss: 0.6611\n",
      "Epoch 8/10\n",
      "671/671 [==============================] - 26s 36ms/step - loss: 0.6944 - val_loss: 0.6593\n",
      "Epoch 9/10\n",
      "671/671 [==============================] - 26s 35ms/step - loss: 0.6923 - val_loss: 0.6577\n",
      "Epoch 10/10\n",
      "671/671 [==============================] - 27s 37ms/step - loss: 0.6906 - val_loss: 0.6564\n",
      "153/153 [==============================] - 1s 6ms/step\n",
      "MAE score for hyperparameters {'N_LAGS': 60, 'BATCH_SIZE': 64, 'DROPOUT': 0.3, 'LEARNING_RATE': 0.0001, 'LSTM_UNITS': 25}: 5.8320136070251465\n",
      "Epoch 1/10\n",
      "671/671 [==============================] - 40s 54ms/step - loss: 0.7365 - val_loss: 0.6845\n",
      "Epoch 2/10\n",
      "671/671 [==============================] - 41s 58ms/step - loss: 0.7196 - val_loss: 0.6740\n",
      "Epoch 3/10\n",
      "671/671 [==============================] - 43s 61ms/step - loss: 0.7087 - val_loss: 0.6676\n",
      "Epoch 4/10\n",
      "671/671 [==============================] - 46s 63ms/step - loss: 0.7011 - val_loss: 0.6625\n",
      "Epoch 5/10\n",
      "671/671 [==============================] - 40s 55ms/step - loss: 0.6953 - val_loss: 0.6583\n",
      "Epoch 6/10\n",
      "671/671 [==============================] - 40s 56ms/step - loss: 0.6905 - val_loss: 0.6549\n",
      "Epoch 7/10\n",
      "671/671 [==============================] - 43s 60ms/step - loss: 0.6868 - val_loss: 0.6519\n",
      "Epoch 8/10\n",
      "671/671 [==============================] - 40s 56ms/step - loss: 0.6837 - val_loss: 0.6493\n",
      "Epoch 9/10\n",
      "671/671 [==============================] - 51s 73ms/step - loss: 0.6810 - val_loss: 0.6469\n",
      "Epoch 10/10\n",
      "671/671 [==============================] - 63s 88ms/step - loss: 0.6787 - val_loss: 0.6448\n",
      "153/153 [==============================] - 2s 10ms/step\n",
      "MAE score for hyperparameters {'N_LAGS': 60, 'BATCH_SIZE': 64, 'DROPOUT': 0.3, 'LEARNING_RATE': 0.0001, 'LSTM_UNITS': 50}: 5.733712196350098\n",
      "Epoch 1/10\n",
      "    107/Unknown - 9s 35ms/step - loss: 0.7274"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[55], line 101\u001b[0m\n\u001b[0;32m     93\u001b[0m model \u001b[38;5;241m=\u001b[39m build_model()\n\u001b[0;32m     95\u001b[0m early_stopping \u001b[38;5;241m=\u001b[39m EarlyStopping(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     96\u001b[0m                                mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     97\u001b[0m                                patience\u001b[38;5;241m=\u001b[39mPATIENCE,\n\u001b[0;32m     98\u001b[0m                                restore_best_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     99\u001b[0m                                verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 101\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    102\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    103\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    104\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhyperparameters\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mBATCH_SIZE\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    105\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    106\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    108\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(valid_dataset)\n\u001b[0;32m    110\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39minverse_transform(y_pred)\n",
      "File \u001b[1;32mc:\\Users\\User\\Alkalmazások\\python\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\User\\Alkalmazások\\python\\lib\\site-packages\\keras\\src\\engine\\training.py:1807\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1799\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1800\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1801\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1804\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1805\u001b[0m ):\n\u001b[0;32m   1806\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1807\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1808\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1809\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\Users\\User\\Alkalmazások\\python\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\User\\Alkalmazások\\python\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:832\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    829\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    831\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 832\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    834\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    835\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\User\\Alkalmazások\\python\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:868\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    865\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    866\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    867\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 868\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    869\u001b[0m \u001b[43m      \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_config\u001b[49m\n\u001b[0;32m    870\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    871\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    872\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    873\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    874\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Users\\User\\Alkalmazások\\python\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\User\\Alkalmazások\\python\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1323\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1319\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1320\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1321\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1322\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1323\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1324\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1325\u001b[0m     args,\n\u001b[0;32m   1326\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1327\u001b[0m     executing_eagerly)\n\u001b[0;32m   1328\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\User\\Alkalmazások\\python\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[1;32mc:\\Users\\User\\Alkalmazások\\python\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[0;32m    261\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\User\\Alkalmazások\\python\\lib\\site-packages\\tensorflow\\python\\eager\\context.py:1486\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1484\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1486\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1487\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1488\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1489\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1490\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1491\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1492\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1493\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1494\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1495\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1496\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1500\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1501\u001b[0m   )\n",
      "File \u001b[1;32mc:\\Users\\User\\Alkalmazások\\python\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Dropout, LSTM, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import itertools\n",
    "\n",
    "SEED = 42\n",
    "N_LAGS = 55\n",
    "BUFFER_SIZE = 100000\n",
    "SPLIT_DAY = 390\n",
    "N_STOCKS = 200\n",
    "N_SECONDS = 55\n",
    "RUN_TRAINING = True\n",
    "PATIENCE = 25\n",
    "EPOCHS = 10\n",
    "\n",
    "# Hyperparameters\n",
    "param_grid = {\n",
    "    'N_LAGS': [55, 60, 65],\n",
    "    'BATCH_SIZE': [32, 64],\n",
    "    'DROPOUT': [0.3, 0.5, 0.7],\n",
    "    'LEARNING_RATE': [1e-4, 1e-3],\n",
    "    'LSTM_UNITS': [25, 50]\n",
    "}\n",
    "\n",
    "best_mae = float('inf')\n",
    "best_hyperparameters = None\n",
    "\n",
    "def windowed_dataset(dataset, shuffle=True, batch_size=BATCH_SIZE):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(dataset)\n",
    "    dataset = dataset.window(N_LAGS + 1, shift=1, drop_remainder=True)\n",
    "    dataset = dataset.flat_map(lambda window: window.batch(N_LAGS + 1))\n",
    "    dataset = dataset.map(lambda window: (window[:-1], window[-1]))\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "    dataset = dataset.batch(batch_size).prefetch(1)\n",
    "    return dataset\n",
    "\n",
    "def build_features(df):\n",
    "    all_stock_ids = range(N_STOCKS)\n",
    "    all_date_ids = df[\"date_id\"].unique()\n",
    "    all_seconds = [i * 10 for i in range(N_SECONDS)]\n",
    "\n",
    "    multi_index = pd.MultiIndex.from_product([all_stock_ids, all_date_ids, all_seconds],\n",
    "                                             names=['stock_id', 'date_id', 'seconds_in_bucket'])\n",
    "    df_full = df.set_index(['stock_id', 'date_id', 'seconds_in_bucket']).reindex(multi_index)\n",
    "    df_full = df_full.fillna(0)\n",
    "    df_full = df_full.reset_index()\n",
    "\n",
    "    df_pivoted = df_full.pivot_table(\n",
    "        values='target',\n",
    "        index=['date_id', 'seconds_in_bucket'],\n",
    "        columns='stock_id')\n",
    "\n",
    "    df_pivoted = df_pivoted.reset_index(drop=True)\n",
    "    df_pivoted.columns.name = None\n",
    "\n",
    "    return df_pivoted\n",
    "\n",
    "if RUN_TRAINING:\n",
    "    split = df_train['date_id'] > SPLIT_DAY\n",
    "    df_train_ = df_train[~split]\n",
    "    df_valid = df_train[split]\n",
    "\n",
    "    df_train_features = build_features(df_train_)\n",
    "    df_valid_features = build_features(df_valid)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    train_features = scaler.fit_transform(df_train_features)\n",
    "    valid_features = scaler.transform(df_valid_features)\n",
    "\n",
    "    train_dataset = windowed_dataset(train_features)\n",
    "    valid_dataset = windowed_dataset(valid_features, shuffle=False)\n",
    "\n",
    "    for params in itertools.product(*param_grid.values()):\n",
    "        hyperparameters = dict(zip(param_grid.keys(), params))\n",
    "\n",
    "        def build_model(dropout=hyperparameters['DROPOUT'], lstm_units=hyperparameters['LSTM_UNITS']):\n",
    "            model = Sequential()\n",
    "            model.add(Input(shape=(hyperparameters['N_LAGS'], N_STOCKS)))\n",
    "            model.add(Dropout(dropout))\n",
    "            model.add(LSTM(lstm_units, return_sequences=False))\n",
    "            model.add(Dropout(dropout))\n",
    "            model.add(Dense(N_STOCKS))\n",
    "            model.compile(loss='mae',\n",
    "                          optimizer=tf.keras.optimizers.Adam(learning_rate=hyperparameters['LEARNING_RATE']))\n",
    "            return model\n",
    "\n",
    "        model = build_model()\n",
    "\n",
    "        early_stopping = EarlyStopping(monitor='val_loss',\n",
    "                                       mode='min',\n",
    "                                       patience=PATIENCE,\n",
    "                                       restore_best_weights=True,\n",
    "                                       verbose=True)\n",
    "\n",
    "        history = model.fit(train_dataset,\n",
    "                            validation_data=valid_dataset,\n",
    "                            epochs=EPOCHS,\n",
    "                            batch_size=hyperparameters['BATCH_SIZE'],\n",
    "                            callbacks=[early_stopping],\n",
    "                            verbose=True)\n",
    "\n",
    "        y_pred = model.predict(valid_dataset)\n",
    "\n",
    "        y_pred = scaler.inverse_transform(y_pred)\n",
    "        y_true = df_valid_features[N_LAGS:]\n",
    "\n",
    "        mae = mean_absolute_error(y_true, y_pred)\n",
    "        print(f\"MAE score for hyperparameters {hyperparameters}: {mae}\")\n",
    "\n",
    "        if mae < best_mae:\n",
    "            best_mae = mae\n",
    "            best_hyperparameters = hyperparameters\n",
    "\n",
    "    print(f\"Best hyperparameters: {best_hyperparameters}, Best MAE: {best_mae}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best Training parameters\n",
    "Epoch 1/10\n",
    "671/671 [==============================] - 50s 69ms/step - loss: 0.7005 - val_loss: 0.6466\n",
    "Epoch 2/10\n",
    "671/671 [==============================] - 39s 55ms/step - loss: 0.6756 - val_loss: 0.6352\n",
    "Epoch 3/10\n",
    "671/671 [==============================] - 51s 73ms/step - loss: 0.6691 - val_loss: 0.6302\n",
    "Epoch 4/10\n",
    "671/671 [==============================] - 65s 93ms/step - loss: 0.6661 - val_loss: 0.6276\n",
    "Epoch 5/10\n",
    "671/671 [==============================] - 43s 60ms/step - loss: 0.6641 - val_loss: 0.6256\n",
    "Epoch 6/10\n",
    "671/671 [==============================] - 35s 49ms/step - loss: 0.6626 - val_loss: 0.6250\n",
    "Epoch 7/10\n",
    "671/671 [==============================] - 40s 57ms/step - loss: 0.6613 - val_loss: 0.6241\n",
    "Epoch 8/10\n",
    "671/671 [==============================] - 39s 55ms/step - loss: 0.6605 - val_loss: 0.6231\n",
    "Epoch 9/10\n",
    "671/671 [==============================] - 48s 68ms/step - loss: 0.6598 - val_loss: 0.6229\n",
    "Epoch 10/10\n",
    "671/671 [==============================] - 36s 50ms/step - loss: 0.6594 - val_loss: 0.6221\n",
    "153/153 [==============================] - 2s 8ms/step\n",
    "MAE score for hyperparameters {'N_LAGS': 60, 'BATCH_SIZE': 32, 'DROPOUT': 0.3, 'LEARNING_RATE': 0.001, 'LSTM_UNITS': 50}: 5.548815727233887\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "N_LAGS = 60\n",
    "BATCH_SIZE = 32\n",
    "BUFFER_SIZE = 100000\n",
    "EPOCHS = 10\n",
    "PATIENCE = 25\n",
    "DROPOUT = 0.3\n",
    "LEARNING_RATE = 1e-3\n",
    "SPLIT_DAY = 390\n",
    "N_STOCKS = 200\n",
    "N_DATES = 481\n",
    "N_SECONDS = 55\n",
    "RUN_TRAINING = True\n",
    "# RUN_FOR_SUBMISSION = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>190</th>\n",
       "      <th>191</th>\n",
       "      <th>192</th>\n",
       "      <th>193</th>\n",
       "      <th>194</th>\n",
       "      <th>195</th>\n",
       "      <th>196</th>\n",
       "      <th>197</th>\n",
       "      <th>198</th>\n",
       "      <th>199</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-3.029704</td>\n",
       "      <td>-5.519986</td>\n",
       "      <td>-8.389950</td>\n",
       "      <td>-4.010201</td>\n",
       "      <td>-7.349849</td>\n",
       "      <td>6.779432</td>\n",
       "      <td>-2.499819</td>\n",
       "      <td>-1.959801</td>\n",
       "      <td>-5.970001</td>\n",
       "      <td>7.970333</td>\n",
       "      <td>...</td>\n",
       "      <td>6.370544</td>\n",
       "      <td>11.940002</td>\n",
       "      <td>-11.529922</td>\n",
       "      <td>-6.489754</td>\n",
       "      <td>3.999472</td>\n",
       "      <td>-0.690222</td>\n",
       "      <td>-0.810027</td>\n",
       "      <td>-8.440018</td>\n",
       "      <td>-0.510216</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.389814</td>\n",
       "      <td>-1.620054</td>\n",
       "      <td>-7.460117</td>\n",
       "      <td>-1.040101</td>\n",
       "      <td>-6.719828</td>\n",
       "      <td>7.710457</td>\n",
       "      <td>-4.280210</td>\n",
       "      <td>-8.010268</td>\n",
       "      <td>-0.780225</td>\n",
       "      <td>-0.640154</td>\n",
       "      <td>...</td>\n",
       "      <td>4.210472</td>\n",
       "      <td>2.599955</td>\n",
       "      <td>-15.599728</td>\n",
       "      <td>-1.749992</td>\n",
       "      <td>-9.030104</td>\n",
       "      <td>-1.320243</td>\n",
       "      <td>-3.259778</td>\n",
       "      <td>-4.410148</td>\n",
       "      <td>-1.419783</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.220009</td>\n",
       "      <td>-6.459951</td>\n",
       "      <td>-6.380081</td>\n",
       "      <td>-2.030134</td>\n",
       "      <td>-0.690222</td>\n",
       "      <td>1.009703</td>\n",
       "      <td>2.930164</td>\n",
       "      <td>-12.069941</td>\n",
       "      <td>-4.119873</td>\n",
       "      <td>-3.250241</td>\n",
       "      <td>...</td>\n",
       "      <td>5.379915</td>\n",
       "      <td>-5.970001</td>\n",
       "      <td>-12.710094</td>\n",
       "      <td>2.160072</td>\n",
       "      <td>-9.999871</td>\n",
       "      <td>5.890131</td>\n",
       "      <td>-0.029802</td>\n",
       "      <td>-6.819963</td>\n",
       "      <td>2.599955</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.450249</td>\n",
       "      <td>-5.149841</td>\n",
       "      <td>-5.819798</td>\n",
       "      <td>1.130104</td>\n",
       "      <td>-2.589822</td>\n",
       "      <td>-3.089905</td>\n",
       "      <td>0.560284</td>\n",
       "      <td>-6.909966</td>\n",
       "      <td>-1.090169</td>\n",
       "      <td>-6.759763</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.269911</td>\n",
       "      <td>1.300573</td>\n",
       "      <td>-16.660095</td>\n",
       "      <td>4.090071</td>\n",
       "      <td>-9.340048</td>\n",
       "      <td>7.020235</td>\n",
       "      <td>1.549721</td>\n",
       "      <td>1.690388</td>\n",
       "      <td>3.010035</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.169775</td>\n",
       "      <td>-6.750226</td>\n",
       "      <td>-0.280142</td>\n",
       "      <td>-2.350211</td>\n",
       "      <td>-4.900098</td>\n",
       "      <td>-2.070069</td>\n",
       "      <td>2.199411</td>\n",
       "      <td>-1.209974</td>\n",
       "      <td>-3.100038</td>\n",
       "      <td>-8.199811</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.009869</td>\n",
       "      <td>-0.969768</td>\n",
       "      <td>-9.229779</td>\n",
       "      <td>5.700588</td>\n",
       "      <td>-11.489987</td>\n",
       "      <td>4.500151</td>\n",
       "      <td>-1.350045</td>\n",
       "      <td>-1.929998</td>\n",
       "      <td>4.669428</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26450</th>\n",
       "      <td>3.999472</td>\n",
       "      <td>2.850294</td>\n",
       "      <td>-4.450083</td>\n",
       "      <td>1.720190</td>\n",
       "      <td>1.939535</td>\n",
       "      <td>-4.569888</td>\n",
       "      <td>-0.630021</td>\n",
       "      <td>-1.729727</td>\n",
       "      <td>-0.680089</td>\n",
       "      <td>3.540516</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.730225</td>\n",
       "      <td>-7.420182</td>\n",
       "      <td>2.959967</td>\n",
       "      <td>-2.359748</td>\n",
       "      <td>-0.650287</td>\n",
       "      <td>3.190041</td>\n",
       "      <td>-1.659989</td>\n",
       "      <td>-0.690222</td>\n",
       "      <td>-2.700090</td>\n",
       "      <td>-7.209778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26451</th>\n",
       "      <td>3.190041</td>\n",
       "      <td>3.169775</td>\n",
       "      <td>-5.559921</td>\n",
       "      <td>4.669428</td>\n",
       "      <td>2.139807</td>\n",
       "      <td>1.679659</td>\n",
       "      <td>-0.360012</td>\n",
       "      <td>-3.240108</td>\n",
       "      <td>2.609491</td>\n",
       "      <td>1.679659</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.340244</td>\n",
       "      <td>-3.479719</td>\n",
       "      <td>1.419783</td>\n",
       "      <td>-1.069903</td>\n",
       "      <td>2.319813</td>\n",
       "      <td>3.770590</td>\n",
       "      <td>-3.039837</td>\n",
       "      <td>-3.299713</td>\n",
       "      <td>-2.070069</td>\n",
       "      <td>-9.750128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26452</th>\n",
       "      <td>-0.169873</td>\n",
       "      <td>5.689859</td>\n",
       "      <td>-5.310178</td>\n",
       "      <td>5.639791</td>\n",
       "      <td>1.660585</td>\n",
       "      <td>2.110004</td>\n",
       "      <td>-1.000166</td>\n",
       "      <td>-4.529953</td>\n",
       "      <td>5.409718</td>\n",
       "      <td>0.350475</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.549721</td>\n",
       "      <td>-1.289845</td>\n",
       "      <td>-0.140071</td>\n",
       "      <td>-1.929998</td>\n",
       "      <td>4.609823</td>\n",
       "      <td>1.449585</td>\n",
       "      <td>-6.319880</td>\n",
       "      <td>-1.999736</td>\n",
       "      <td>-2.239943</td>\n",
       "      <td>3.629923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26453</th>\n",
       "      <td>3.110170</td>\n",
       "      <td>10.650158</td>\n",
       "      <td>-5.239844</td>\n",
       "      <td>5.229712</td>\n",
       "      <td>0.300407</td>\n",
       "      <td>-2.030134</td>\n",
       "      <td>-1.180172</td>\n",
       "      <td>-1.310110</td>\n",
       "      <td>3.240108</td>\n",
       "      <td>0.220537</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.440048</td>\n",
       "      <td>-0.299811</td>\n",
       "      <td>-2.120137</td>\n",
       "      <td>-1.350045</td>\n",
       "      <td>3.110170</td>\n",
       "      <td>4.019737</td>\n",
       "      <td>-7.349849</td>\n",
       "      <td>-3.259778</td>\n",
       "      <td>-3.160238</td>\n",
       "      <td>4.760027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26454</th>\n",
       "      <td>0.760555</td>\n",
       "      <td>15.859604</td>\n",
       "      <td>-1.549721</td>\n",
       "      <td>3.160238</td>\n",
       "      <td>1.009703</td>\n",
       "      <td>2.340078</td>\n",
       "      <td>1.410246</td>\n",
       "      <td>-1.260042</td>\n",
       "      <td>3.679991</td>\n",
       "      <td>-1.609921</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.470281</td>\n",
       "      <td>0.799894</td>\n",
       "      <td>-1.729727</td>\n",
       "      <td>-3.880262</td>\n",
       "      <td>4.839897</td>\n",
       "      <td>2.310276</td>\n",
       "      <td>-8.220077</td>\n",
       "      <td>1.169443</td>\n",
       "      <td>-1.540184</td>\n",
       "      <td>-6.530285</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26455 rows × 200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0          1         2         3         4         5         6    \\\n",
       "0     -3.029704  -5.519986 -8.389950 -4.010201 -7.349849  6.779432 -2.499819   \n",
       "1      0.389814  -1.620054 -7.460117 -1.040101 -6.719828  7.710457 -4.280210   \n",
       "2      4.220009  -6.459951 -6.380081 -2.030134 -0.690222  1.009703  2.930164   \n",
       "3      5.450249  -5.149841 -5.819798  1.130104 -2.589822 -3.089905  0.560284   \n",
       "4      3.169775  -6.750226 -0.280142 -2.350211 -4.900098 -2.070069  2.199411   \n",
       "...         ...        ...       ...       ...       ...       ...       ...   \n",
       "26450  3.999472   2.850294 -4.450083  1.720190  1.939535 -4.569888 -0.630021   \n",
       "26451  3.190041   3.169775 -5.559921  4.669428  2.139807  1.679659 -0.360012   \n",
       "26452 -0.169873   5.689859 -5.310178  5.639791  1.660585  2.110004 -1.000166   \n",
       "26453  3.110170  10.650158 -5.239844  5.229712  0.300407 -2.030134 -1.180172   \n",
       "26454  0.760555  15.859604 -1.549721  3.160238  1.009703  2.340078  1.410246   \n",
       "\n",
       "             7         8         9    ...       190        191        192  \\\n",
       "0      -1.959801 -5.970001  7.970333  ...  6.370544  11.940002 -11.529922   \n",
       "1      -8.010268 -0.780225 -0.640154  ...  4.210472   2.599955 -15.599728   \n",
       "2     -12.069941 -4.119873 -3.250241  ...  5.379915  -5.970001 -12.710094   \n",
       "3      -6.909966 -1.090169 -6.759763  ... -3.269911   1.300573 -16.660095   \n",
       "4      -1.209974 -3.100038 -8.199811  ... -2.009869  -0.969768  -9.229779   \n",
       "...          ...       ...       ...  ...       ...        ...        ...   \n",
       "26450  -1.729727 -0.680089  3.540516  ... -4.730225  -7.420182   2.959967   \n",
       "26451  -3.240108  2.609491  1.679659  ... -3.340244  -3.479719   1.419783   \n",
       "26452  -4.529953  5.409718  0.350475  ... -1.549721  -1.289845  -0.140071   \n",
       "26453  -1.310110  3.240108  0.220537  ... -1.440048  -0.299811  -2.120137   \n",
       "26454  -1.260042  3.679991 -1.609921  ... -0.470281   0.799894  -1.729727   \n",
       "\n",
       "            193        194       195       196       197       198       199  \n",
       "0     -6.489754   3.999472 -0.690222 -0.810027 -8.440018 -0.510216  0.000000  \n",
       "1     -1.749992  -9.030104 -1.320243 -3.259778 -4.410148 -1.419783  0.000000  \n",
       "2      2.160072  -9.999871  5.890131 -0.029802 -6.819963  2.599955  0.000000  \n",
       "3      4.090071  -9.340048  7.020235  1.549721  1.690388  3.010035  0.000000  \n",
       "4      5.700588 -11.489987  4.500151 -1.350045 -1.929998  4.669428  0.000000  \n",
       "...         ...        ...       ...       ...       ...       ...       ...  \n",
       "26450 -2.359748  -0.650287  3.190041 -1.659989 -0.690222 -2.700090 -7.209778  \n",
       "26451 -1.069903   2.319813  3.770590 -3.039837 -3.299713 -2.070069 -9.750128  \n",
       "26452 -1.929998   4.609823  1.449585 -6.319880 -1.999736 -2.239943  3.629923  \n",
       "26453 -1.350045   3.110170  4.019737 -7.349849 -3.259778 -3.160238  4.760027  \n",
       "26454 -3.880262   4.839897  2.310276 -8.220077  1.169443 -1.540184 -6.530285  \n",
       "\n",
       "[26455 rows x 200 columns]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "build_features(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "671/671 [==============================] - 87s 103ms/step - loss: 0.7065 - val_loss: 0.6570\n",
      "Epoch 2/10\n",
      "671/671 [==============================] - 67s 87ms/step - loss: 0.6877 - val_loss: 0.6497\n",
      "Epoch 3/10\n",
      "671/671 [==============================] - 67s 86ms/step - loss: 0.6833 - val_loss: 0.6469\n",
      "Epoch 4/10\n",
      "671/671 [==============================] - 68s 86ms/step - loss: 0.6807 - val_loss: 0.6447\n",
      "Epoch 5/10\n",
      "671/671 [==============================] - 68s 87ms/step - loss: 0.6792 - val_loss: 0.6437\n",
      "Epoch 6/10\n",
      "671/671 [==============================] - 68s 86ms/step - loss: 0.6779 - val_loss: 0.6425\n",
      "Epoch 7/10\n",
      "671/671 [==============================] - 68s 87ms/step - loss: 0.6771 - val_loss: 0.6417\n",
      "Epoch 8/10\n",
      "671/671 [==============================] - 67s 85ms/step - loss: 0.6761 - val_loss: 0.6410\n",
      "Epoch 9/10\n",
      "671/671 [==============================] - 68s 87ms/step - loss: 0.6753 - val_loss: 0.6402\n",
      "Epoch 10/10\n",
      "671/671 [==============================] - 68s 85ms/step - loss: 0.6746 - val_loss: 0.6400\n",
      "153/153 [==============================] - 4s 23ms/step\n",
      "MAE score: 5.703173637390137\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Dropout, LSTM, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import itertools\n",
    "\n",
    "SEED = 42\n",
    "N_LAGS = 60\n",
    "BATCH_SIZE = 32\n",
    "BUFFER_SIZE = 100000\n",
    "EPOCHS = 10\n",
    "PATIENCE = 25\n",
    "DROPOUT = 0.3\n",
    "LEARNING_RATE = 1e-3\n",
    "SPLIT_DAY = 390\n",
    "N_STOCKS = 200\n",
    "N_DATES = 481\n",
    "N_SECONDS = 55\n",
    "RUN_TRAINING = True\n",
    "\n",
    "\n",
    "best_mae = float('inf')\n",
    "best_hyperparameters = None\n",
    "\n",
    "def windowed_dataset(dataset, shuffle=True, batch_size=BATCH_SIZE):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(dataset)\n",
    "    dataset = dataset.window(N_LAGS + 1, shift=1, drop_remainder=True)\n",
    "    dataset = dataset.flat_map(lambda window: window.batch(N_LAGS + 1))\n",
    "    dataset = dataset.map(lambda window: (window[:-1], window[-1]))\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "    dataset = dataset.batch(batch_size).prefetch(1)\n",
    "    return dataset\n",
    "\n",
    "def build_features(df):\n",
    "    all_stock_ids = range(N_STOCKS)\n",
    "    all_date_ids = df[\"date_id\"].unique()\n",
    "    all_seconds = [i * 10 for i in range(N_SECONDS)]\n",
    "\n",
    "    multi_index = pd.MultiIndex.from_product([all_stock_ids, all_date_ids, all_seconds],\n",
    "                                             names=['stock_id', 'date_id', 'seconds_in_bucket'])\n",
    "    df_full = df.set_index(['stock_id', 'date_id', 'seconds_in_bucket']).reindex(multi_index)\n",
    "    df_full = df_full.fillna(0)\n",
    "    df_full = df_full.reset_index()\n",
    "\n",
    "    df_pivoted = df_full.pivot_table(\n",
    "        values='target',\n",
    "        index=['date_id', 'seconds_in_bucket'],\n",
    "        columns='stock_id')\n",
    "\n",
    "    df_pivoted = df_pivoted.reset_index(drop=True)\n",
    "    df_pivoted.columns.name = None\n",
    "\n",
    "    return df_pivoted\n",
    "\n",
    "if RUN_TRAINING:\n",
    "\n",
    "  split = df_train['date_id'] > SPLIT_DAY\n",
    "  df_train_ = df_train[~split]\n",
    "  df_valid = df_train[split]\n",
    "\n",
    "  df_train_features = build_features(df_train_)\n",
    "  df_valid_features = build_features(df_valid)\n",
    "\n",
    "  scaler = StandardScaler()\n",
    "  train_features = scaler.fit_transform(df_train_features)\n",
    "  valid_features = scaler.transform(df_valid_features)\n",
    "\n",
    "  train_dataset = windowed_dataset(train_features)\n",
    "  valid_dataset = windowed_dataset(valid_features, shuffle=False)\n",
    "\n",
    "  model = build_model()\n",
    "\n",
    "  early_stopping = EarlyStopping(monitor='val_loss',\n",
    "                    mode='min',\n",
    "                    patience=PATIENCE,\n",
    "                    restore_best_weights=True,\n",
    "                    verbose=True)\n",
    "\n",
    "  history = model.fit(train_dataset,\n",
    "                      validation_data=valid_dataset,\n",
    "                      epochs=EPOCHS,\n",
    "                      batch_size=BATCH_SIZE,\n",
    "                      callbacks=[early_stopping],\n",
    "                      verbose=True)\n",
    "\n",
    "  ## Evaluate ##\n",
    "  y_pred = model.predict(valid_dataset)\n",
    "\n",
    "  y_pred = scaler.inverse_transform(y_pred)\n",
    "  y_true = df_valid_features[N_LAGS:]\n",
    "\n",
    "  mae = mean_absolute_error(y_true, y_pred)\n",
    "  print(f\"MAE score: {mae}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plots ##\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model train vs validation loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper right')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
